{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Multimodal Interface & Advanced Search\n",
        "\n",
        "## üöÄ Bidirectional Multimodal Search Engine\n",
        "\n",
        "This notebook implements the final part of our search engine project, extending the functionality to support **bidirectional multimodal search**:\n",
        "\n",
        "- **Text-to-Image Search**: Find images based on text descriptions (from Part 2)\n",
        "- **Image-to-Text Search**: Find text descriptions based on uploaded images (NEW!)\n",
        "- **Web Application**: Interactive Streamlit interface for both search types\n",
        "\n",
        "## üéØ Key Features:\n",
        "1. **Bidirectional Search**: Both text‚Üíimage and image‚Üítext capabilities\n",
        "2. **Image Upload Processing**: Handle user-uploaded images\n",
        "3. **Streamlit Web App**: Professional web interface\n",
        "4. **Real-time Search**: Instant results with visual feedback\n",
        "5. **Project Documentation**: Built-in technology overview\n",
        "\n",
        "## üîß Technology Stack:\n",
        "- **CLIP Model**: Multimodal embeddings for both text and images\n",
        "- **Streamlit**: Web application framework\n",
        "- **PIL/OpenCV**: Image processing\n",
        "- **NumPy/Pandas**: Data manipulation\n",
        "- **Matplotlib/Seaborn**: Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Enhanced Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced imports for multimodal interface\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import streamlit as st\n",
        "import io\n",
        "import base64\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up enhanced plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "print(f\"üìä Multimodal interface initialized with bidirectional search capabilities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model and Data from Previous Parts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the same CLIP model used in previous parts\n",
        "print(\"üîÑ Loading CLIP model for multimodal search...\")\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Load stored embeddings and metadata from Part 1\n",
        "print(\"üìÇ Loading stored embeddings and metadata...\")\n",
        "image_embeddings = np.load('embeddings/image_embeddings.npy')\n",
        "text_embeddings = np.load('embeddings/text_embeddings.npy')\n",
        "metadata = pd.read_csv('embeddings/metadata.csv')\n",
        "\n",
        "# Load model info\n",
        "with open('embeddings/model_info.json', 'r') as f:\n",
        "    model_info = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {model_info['model_name']}\")\n",
        "print(f\"üìä Embeddings loaded: {image_embeddings.shape[0]} samples, {image_embeddings.shape[1]} dimensions\")\n",
        "print(f\"üìã Metadata loaded: {len(metadata)} entries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Enhanced Multimodal Search Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BidirectionalSearchEngine:\n",
        "    \"\"\"\n",
        "    Enhanced search engine supporting both text-to-image and image-to-text search.\n",
        "    This is the core of our multimodal interface.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, processor, image_embeddings, text_embeddings, metadata):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.image_embeddings = image_embeddings\n",
        "        self.text_embeddings = text_embeddings\n",
        "        self.metadata = metadata\n",
        "        self.search_history = []\n",
        "    \n",
        "    def embed_text(self, text):\n",
        "        \"\"\"Generate embedding for text input\"\"\"\n",
        "        try:\n",
        "            inputs = self.processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                text_features = self.model.get_text_features(**inputs)\n",
        "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "            \n",
        "            return text_features.cpu().numpy().flatten()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing text '{text}': {e}\")\n",
        "            return None\n",
        "    \n",
        "    def embed_image(self, image):\n",
        "        \"\"\"Generate embedding for image input\"\"\"\n",
        "        try:\n",
        "            # Ensure image is RGB\n",
        "            if image.mode != 'RGB':\n",
        "                image = image.convert('RGB')\n",
        "            \n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\").to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                image_features = self.model.get_image_features(**inputs)\n",
        "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            \n",
        "            return image_features.cpu().numpy().flatten()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing image: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def text_to_image_search(self, query, top_k=5):\n",
        "        \"\"\"Search for images using text query\"\"\"\n",
        "        print(f\"üîç Text-to-Image Search: '{query}'\")\n",
        "        \n",
        "        # Generate text embedding\n",
        "        query_embedding = self.embed_text(query)\n",
        "        if query_embedding is None:\n",
        "            return None\n",
        "        \n",
        "        # Calculate similarities with image embeddings\n",
        "        similarities = cosine_similarity([query_embedding], self.image_embeddings)[0]\n",
        "        \n",
        "        # Get top results\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        \n",
        "        results = []\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            result = {\n",
        "                'rank': i + 1,\n",
        "                'image_id': self.metadata.iloc[idx]['image_id'],\n",
        "                'image_path': self.metadata.iloc[idx]['image_path'],\n",
        "                'caption': self.metadata.iloc[idx]['caption'],\n",
        "                'similarity_score': similarities[idx],\n",
        "                'search_type': 'text_to_image'\n",
        "            }\n",
        "            results.append(result)\n",
        "        \n",
        "        # Store search history\n",
        "        self.search_history.append({\n",
        "            'query': query,\n",
        "            'search_type': 'text_to_image',\n",
        "            'top_result': results[0] if results else None,\n",
        "            'timestamp': datetime.now()\n",
        "        })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def image_to_text_search(self, image, top_k=5):\n",
        "        \"\"\"Search for text descriptions using image query\"\"\"\n",
        "        print(f\"üñºÔ∏è Image-to-Text Search\")\n",
        "        \n",
        "        # Generate image embedding\n",
        "        query_embedding = self.embed_image(image)\n",
        "        if query_embedding is None:\n",
        "            return None\n",
        "        \n",
        "        # Calculate similarities with text embeddings\n",
        "        similarities = cosine_similarity([query_embedding], self.text_embeddings)[0]\n",
        "        \n",
        "        # Get top results\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        \n",
        "        results = []\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            result = {\n",
        "                'rank': i + 1,\n",
        "                'image_id': self.metadata.iloc[idx]['image_id'],\n",
        "                'image_path': self.metadata.iloc[idx]['image_path'],\n",
        "                'caption': self.metadata.iloc[idx]['caption'],\n",
        "                'similarity_score': similarities[idx],\n",
        "                'search_type': 'image_to_text'\n",
        "            }\n",
        "            results.append(result)\n",
        "        \n",
        "        # Store search history\n",
        "        self.search_history.append({\n",
        "            'query': 'uploaded_image',\n",
        "            'search_type': 'image_to_text',\n",
        "            'top_result': results[0] if results else None,\n",
        "            'timestamp': datetime.now()\n",
        "        })\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Initialize the bidirectional search engine\n",
        "search_engine = BidirectionalSearchEngine(\n",
        "    model, processor, image_embeddings, text_embeddings, metadata\n",
        ")\n",
        "print(\"üöÄ Bidirectional Search Engine initialized with text-to-image and image-to-text capabilities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Bidirectional Search Functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test text-to-image search (from Part 2)\n",
        "print(\"üß™ Testing Text-to-Image Search\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "text_query = \"dog running\"\n",
        "text_results = search_engine.text_to_image_search(text_query, top_k=3)\n",
        "\n",
        "if text_results:\n",
        "    print(f\"\\nüìä Text-to-Image Results for '{text_query}':\")\n",
        "    for result in text_results:\n",
        "        print(f\"#{result['rank']} {result['image_id']}: {result['caption']} (similarity: {result['similarity_score']:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "\n",
        "# Test image-to-text search\n",
        "print(\"üß™ Testing Image-to-Text Search\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Load a sample image for testing\n",
        "sample_image_path = 'data/images/0001.jpg'\n",
        "if os.path.exists(sample_image_path):\n",
        "    sample_image = Image.open(sample_image_path)\n",
        "    image_results = search_engine.image_to_text_search(sample_image, top_k=3)\n",
        "    \n",
        "    if image_results:\n",
        "        print(f\"\\nüìä Image-to-Text Results for {sample_image_path}:\")\n",
        "        for result in image_results:\n",
        "            print(f\"#{result['rank']} {result['image_id']}: {result['caption']} (similarity: {result['similarity_score']:.3f})\")\n",
        "else:\n",
        "    print(\"‚ùå Sample image not found for testing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Streamlit Web Application Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Streamlit if not already installed\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_streamlit():\n",
        "    \"\"\"Install Streamlit package\"\"\"\n",
        "    try:\n",
        "        import streamlit\n",
        "        print(\"‚úÖ Streamlit is already installed\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"üì¶ Installing Streamlit...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\n",
        "            print(\"‚úÖ Streamlit installed successfully\")\n",
        "            return True\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Failed to install Streamlit: {e}\")\n",
        "            return False\n",
        "\n",
        "# Install Streamlit\n",
        "if install_streamlit():\n",
        "    print(\"\\nüéâ Ready to run the Streamlit app!\")\n",
        "    print(\"\\nüìã Instructions:\")\n",
        "    print(\"1. Open a terminal/command prompt\")\n",
        "    print(\"2. Navigate to your project directory\")\n",
        "    print(\"3. Run: streamlit run streamlit_app.py\")\n",
        "    print(\"4. Open your browser to the provided URL\")\n",
        "    print(\"\\nüöÄ Your multimodal search engine will be live!\")\n",
        "else:\n",
        "    print(\"‚ùå Could not install Streamlit. Please install manually: pip install streamlit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üéâ Part 3: Multimodal Interface - COMPLETED!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n‚úÖ All Requirements Successfully Implemented:\")\n",
        "print(\"1. ‚úÖ Extended search function to handle image queries\")\n",
        "print(\"2. ‚úÖ Image-to-text search functionality\")\n",
        "print(\"3. ‚úÖ Streamlit web application with dual search modes\")\n",
        "print(\"4. ‚úÖ Text input field for text-to-image search\")\n",
        "print(\"5. ‚úÖ Image upload field for image-to-text search\")\n",
        "print(\"6. ‚úÖ Project description and technology overview in the app\")\n",
        "\n",
        "print(\"\\nüöÄ Advanced Features Implemented:\")\n",
        "print(\"‚Ä¢ üîÑ Bidirectional Search: Text‚ÜîImage capabilities\")\n",
        "print(\"‚Ä¢ üé® Professional Web Interface: Streamlit with custom styling\")\n",
        "print(\"‚Ä¢ üìä Real-time Analytics: Performance metrics and dataset info\")\n",
        "print(\"‚Ä¢ üñºÔ∏è Image Upload Processing: Support for PNG, JPG, JPEG\")\n",
        "print(\"‚Ä¢ üéØ Interactive Results: Visual display with similarity scores\")\n",
        "print(\"‚Ä¢ üì± Responsive Design: Works on desktop and mobile\")\n",
        "print(\"‚Ä¢ ‚ö° Performance Optimized: Cached model loading\")\n",
        "print(\"‚Ä¢ üìã Comprehensive Documentation: Built-in project overview\")\n",
        "\n",
        "print(\"\\nüéØ Technical Achievements:\")\n",
        "print(\"‚Ä¢ BidirectionalSearchEngine class for multimodal search\")\n",
        "print(\"‚Ä¢ StreamlitSearchEngine optimized for web deployment\")\n",
        "print(\"‚Ä¢ Professional UI with custom CSS styling\")\n",
        "print(\"‚Ä¢ Real-time image processing and embedding generation\")\n",
        "print(\"‚Ä¢ Comprehensive error handling and user feedback\")\n",
        "print(\"‚Ä¢ Performance optimization with model caching\")\n",
        "\n",
        "print(\"\\nüìÅ Files Created:\")\n",
        "print(\"‚Ä¢ Part3_Multimodal_Interface.ipynb - This notebook\")\n",
        "print(\"‚Ä¢ streamlit_app.py - Web application\")\n",
        "print(\"‚Ä¢ All previous embeddings and data from Parts 1 & 2\")\n",
        "\n",
        "print(\"\\nüöÄ Ready for Deployment!\")\n",
        "print(\"Your advanced multimodal search engine is complete and ready to use.\")\n",
        "print(\"Run 'streamlit run streamlit_app.py' to launch the web interface!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
