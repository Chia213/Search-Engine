{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Multimodal & Interface Upgrade (Advanced continuation)\n",
        "\n",
        "## Project Overview\n",
        "This notebook demonstrates the creation of both Streamlit and Gradio web applications for our multimodal search engine. The search engine can:\n",
        "- Find images using text descriptions (text-to-image search)\n",
        "- Find text descriptions using uploaded images (image-to-text search)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. System Information and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîç MULTIMODAL INTERFACE - SYSTEM STATUS\n",
            "================================================================================\n",
            "üìÖ Timestamp: 2025-09-11 00:49:55\n",
            "\n",
            "üñ•Ô∏è  SYSTEM INFORMATION\n",
            "----------------------------------------\n",
            "Platform: Windows-11-10.0.26100-SP0\n",
            "Architecture: 64bit\n",
            "Processor: Intel64 Family 6 Model 151 Stepping 5, GenuineIntel\n",
            "Python Version: 3.12.9\n",
            "PyTorch Version: 2.8.0+cpu\n",
            "Streamlit Version: 1.49.1\n",
            "\n",
            "‚ö° HARDWARE INFORMATION\n",
            "----------------------------------------\n",
            "CPU Cores: 6 physical, 12 logical\n",
            "RAM: 15.8 GB total, 1.1 GB available\n",
            "RAM Usage: 92.9%\n",
            "Device: cpu\n",
            "GPU: Not available (using CPU)\n",
            "\n",
            "üìÅ PROJECT STATUS\n",
            "----------------------------------------\n",
            "‚úÖ Data directory found\n",
            "‚úÖ 8091 images found\n",
            "   Total image size: 1063.1 MB\n",
            "‚úÖ 40460 captions found (3355.2 KB)\n",
            "‚úÖ Flickr8k token file found (3355.2 KB)\n",
            "‚úÖ Embeddings directory found\n",
            "‚úÖ Image embeddings found (1.0 MB)\n",
            "‚úÖ Text embeddings found (1.0 MB)\n",
            "‚úÖ Metadata found (58.3 KB)\n",
            "‚úÖ Model info found\n",
            "\n",
            "üöÄ READY TO BUILD MULTIMODAL INTERFACE\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "import platform\n",
        "import psutil\n",
        "import sys\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Display comprehensive system information\n",
        "print(\"=\" * 80)\n",
        "print(\"üîç MULTIMODAL INTERFACE - SYSTEM STATUS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print()\n",
        "\n",
        "# System Information\n",
        "print(\"üñ•Ô∏è  SYSTEM INFORMATION\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "print(f\"Architecture: {platform.architecture()[0]}\")\n",
        "print(f\"Processor: {platform.processor()}\")\n",
        "print(f\"Python Version: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Streamlit Version: {st.__version__}\")\n",
        "print()\n",
        "\n",
        "# Hardware Information\n",
        "print(\"‚ö° HARDWARE INFORMATION\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
        "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB total, {psutil.virtual_memory().available / (1024**3):.1f} GB available\")\n",
        "print(f\"RAM Usage: {psutil.virtual_memory().percent:.1f}%\")\n",
        "\n",
        "# GPU Information\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
        "else:\n",
        "    print(\"GPU: Not available (using CPU)\")\n",
        "print()\n",
        "\n",
        "# Project Status\n",
        "print(\"üìÅ PROJECT STATUS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check if data exists\n",
        "data_path = '../data/'\n",
        "if os.path.exists(data_path):\n",
        "    print(\"‚úÖ Data directory found\")\n",
        "    if os.path.exists('../data/images/'):\n",
        "        image_files = [f for f in os.listdir('../data/images/') if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        image_count = len(image_files)\n",
        "        print(f\"‚úÖ {image_count} images found\")\n",
        "        if image_count > 0:\n",
        "            total_size = sum(os.path.getsize(os.path.join('../data/images/', f)) for f in image_files) / (1024**2)\n",
        "            print(f\"   Total image size: {total_size:.1f} MB\")\n",
        "    if os.path.exists('../data/captions.txt'):\n",
        "        with open('../data/captions.txt', 'r') as f:\n",
        "            caption_count = sum(1 for line in f)\n",
        "        caption_size = os.path.getsize('../data/captions.txt') / 1024\n",
        "        print(f\"‚úÖ {caption_count} captions found ({caption_size:.1f} KB)\")\n",
        "    if os.path.exists('../data/Flickr8k.token.txt'):\n",
        "        token_size = os.path.getsize('../data/Flickr8k.token.txt') / 1024\n",
        "        print(f\"‚úÖ Flickr8k token file found ({token_size:.1f} KB)\")\n",
        "else:\n",
        "    print(\"‚ùå Data directory not found!\")\n",
        "\n",
        "# Check if embeddings exist\n",
        "embeddings_path = '../embeddings/'\n",
        "if os.path.exists(embeddings_path):\n",
        "    print(\"‚úÖ Embeddings directory found\")\n",
        "    if os.path.exists('../embeddings/image_embeddings.npy'):\n",
        "        image_emb_size = os.path.getsize('../embeddings/image_embeddings.npy') / (1024**2)\n",
        "        print(f\"‚úÖ Image embeddings found ({image_emb_size:.1f} MB)\")\n",
        "    if os.path.exists('../embeddings/text_embeddings.npy'):\n",
        "        text_emb_size = os.path.getsize('../embeddings/text_embeddings.npy') / (1024**2)\n",
        "        print(f\"‚úÖ Text embeddings found ({text_emb_size:.1f} MB)\")\n",
        "    if os.path.exists('../embeddings/metadata.csv'):\n",
        "        metadata_size = os.path.getsize('../embeddings/metadata.csv') / 1024\n",
        "        print(f\"‚úÖ Metadata found ({metadata_size:.1f} KB)\")\n",
        "    if os.path.exists('../embeddings/model_info.json'):\n",
        "        print(\"‚úÖ Model info found\")\n",
        "else:\n",
        "    print(\"‚ùå Embeddings directory not found - please run Part 1 first!\")\n",
        "\n",
        "print()\n",
        "print(\"üöÄ READY TO BUILD MULTIMODAL INTERFACE\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model and Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è  Using device: cpu\n",
            "üîÑ Loading CLIP model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c01570af8ca433db21e7f3466ba6eab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CLIP model loaded successfully!\n",
            "üîÑ Loading embeddings data...\n",
            "‚úÖ Embeddings data loaded successfully!\n",
            "üìä Image embeddings shape: (500, 512)\n",
            "üìä Text embeddings shape: (500, 512)\n",
            "üìä Metadata shape: (500, 3)\n",
            "üìä Model info: {'model_name': 'openai/clip-vit-base-patch32', 'embedding_dim': 512, 'num_images': 100, 'total_embeddings': 500, 'num_samples': 500, 'dataset': 'Flickr8k (partial - 100/8091 images)', 'processing_date': '2025-09-10', 'device_used': 'cpu', 'note': 'Only 100 unique images processed out of 8091 total images'}\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "# Load CLIP model\n",
        "print(\"üîÑ Loading CLIP model...\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "print(\"‚úÖ CLIP model loaded successfully!\")\n",
        "\n",
        "# Load embeddings data\n",
        "print(\"üîÑ Loading embeddings data...\")\n",
        "image_embeddings = np.load('../embeddings/image_embeddings.npy')\n",
        "text_embeddings = np.load('../embeddings/text_embeddings.npy')\n",
        "metadata = pd.read_csv('../embeddings/metadata.csv')\n",
        "\n",
        "# Load model info\n",
        "with open('../embeddings/model_info.json', 'r') as f:\n",
        "    model_info = json.load(f)\n",
        "\n",
        "print(\"‚úÖ Embeddings data loaded successfully!\")\n",
        "print(f\"üìä Image embeddings shape: {image_embeddings.shape}\")\n",
        "print(f\"üìä Text embeddings shape: {text_embeddings.shape}\")\n",
        "print(f\"üìä Metadata shape: {metadata.shape}\")\n",
        "print(f\"üìä Model info: {model_info}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Search Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Search functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Text-to-Image Search Function\n",
        "def text_to_image_search(query_text, top_k=5):\n",
        "    \"\"\"Search for images based on text query\"\"\"\n",
        "    # Generate embedding for text query\n",
        "    inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_text_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Calculate similarities with all image embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), image_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar images\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Image-to-Text Search Function\n",
        "def image_to_text_search(uploaded_image, top_k=5):\n",
        "    \"\"\"Search for text descriptions based on uploaded image\"\"\"\n",
        "    # Generate embedding for uploaded image\n",
        "    inputs = processor(images=uploaded_image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_image_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Calculate similarities with all text embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), text_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar text descriptions\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Search functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Search Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing text-to-image search...\n",
            "Query: 'a dog playing'\n",
            "Found 3 results:\n",
            "1. Similarity: 0.324\n",
            "   Caption: A black and white dog catches a toy in midair .\n",
            "   Image ID: 1072153132_53d2bb1b60\n",
            "\n",
            "2. Similarity: 0.324\n",
            "   Caption: A multicolor dog jumping to catch a tennis ball in a grassy field .\n",
            "   Image ID: 1072153132_53d2bb1b60\n",
            "\n",
            "3. Similarity: 0.324\n",
            "   Caption: A dog leaps while chasing a tennis ball through a grassy field .\n",
            "   Image ID: 1072153132_53d2bb1b60\n",
            "\n",
            "‚úÖ Text-to-image search test completed!\n"
          ]
        }
      ],
      "source": [
        "# Test text-to-image search\n",
        "print(\"üîç Testing text-to-image search...\")\n",
        "test_query = \"a dog playing\"\n",
        "results = text_to_image_search(test_query, top_k=3)\n",
        "\n",
        "print(f\"Query: '{test_query}'\")\n",
        "print(f\"Found {len(results)} results:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"{i}. Similarity: {result['similarity']:.3f}\")\n",
        "    print(f\"   Caption: {result['caption']}\")\n",
        "    print(f\"   Image ID: {result['image_id']}\")\n",
        "    print()\n",
        "\n",
        "print(\"‚úÖ Text-to-image search test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Standalone Streamlit App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Streamlit app created successfully!\n",
            "üìÅ File saved as: ../streamlit_app.py\n",
            "üöÄ To run: streamlit run ../streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "# Create the complete Streamlit app code\n",
        "streamlit_code = '''\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import io\n",
        "from datetime import datetime\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CLIP model\n",
        "@st.cache_resource\n",
        "def load_clip_model():\n",
        "    \"\"\"Load CLIP model and processor\"\"\"\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    return model, processor\n",
        "\n",
        "# Load embeddings data\n",
        "@st.cache_data\n",
        "def load_embeddings_data():\n",
        "    \"\"\"Load pre-computed embeddings and metadata\"\"\"\n",
        "    # Load embeddings\n",
        "    image_embeddings = np.load('embeddings/image_embeddings.npy')\n",
        "    text_embeddings = np.load('embeddings/text_embeddings.npy')\n",
        "\n",
        "    # Load metadata\n",
        "    metadata = pd.read_csv('embeddings/metadata.csv')\n",
        "\n",
        "    # Load model info\n",
        "    with open('embeddings/model_info.json', 'r') as f:\n",
        "        model_info = json.load(f)\n",
        "\n",
        "    return image_embeddings, text_embeddings, metadata, model_info\n",
        "\n",
        "# Load model and data\n",
        "model, processor = load_clip_model()\n",
        "image_embeddings, text_embeddings, metadata, model_info = load_embeddings_data()\n",
        "\n",
        "# Text-to-Image Search Function\n",
        "def text_to_image_search(query_text, top_k=5):\n",
        "    \"\"\"Search for images based on text query\"\"\"\n",
        "    # Generate embedding for text query\n",
        "    inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_text_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Calculate similarities with all image embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), image_embeddings)[0]\n",
        "\n",
        "    # Get top-k most similar images\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Image-to-Text Search Function\n",
        "def image_to_text_search(uploaded_image, top_k=5):\n",
        "    \"\"\"Search for text descriptions based on uploaded image\"\"\"\n",
        "    # Generate embedding for uploaded image\n",
        "    inputs = processor(images=uploaded_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_image_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Calculate similarities with all text embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), text_embeddings)[0]\n",
        "\n",
        "    # Get top-k most similar text descriptions\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Custom CSS for modern UI\n",
        "def load_css():\n",
        "    st.markdown(\"\"\"\n",
        "    <style>\n",
        "    /* Modern theme colors - Clean & Minimal */\n",
        "    :root {\n",
        "        --primary-color: #2563eb;\n",
        "        --primary-light: #3b82f6;\n",
        "        --primary-dark: #1d4ed8;\n",
        "        --secondary-color: #7c3aed;\n",
        "        --accent-color: #06b6d4;\n",
        "        --success-color: #059669;\n",
        "        --warning-color: #d97706;\n",
        "        --error-color: #dc2626;\n",
        "        --dark-color: #111827;\n",
        "        --dark-light: #374151;\n",
        "        --light-color: #ffffff;\n",
        "        --gray-50: #f9fafb;\n",
        "        --gray-100: #f3f4f6;\n",
        "        --gray-200: #e5e7eb;\n",
        "        --gray-300: #d1d5db;\n",
        "        --gray-400: #9ca3af;\n",
        "        --gray-500: #6b7280;\n",
        "        --gray-600: #4b5563;\n",
        "        --gray-700: #374151;\n",
        "        --gray-800: #1f2937;\n",
        "        --gray-900: #111827;\n",
        "        --gradient-primary: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #7c3aed 100%);\n",
        "        --gradient-secondary: linear-gradient(135deg, #06b6d4 0%, #3b82f6 100%);\n",
        "        --gradient-accent: linear-gradient(135deg, #f59e0b 0%, #f97316 100%);\n",
        "        --shadow-xs: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\n",
        "        --shadow-sm: 0 1px 3px 0 rgba(0, 0, 0, 0.1), 0 1px 2px 0 rgba(0, 0, 0, 0.06);\n",
        "        --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);\n",
        "        --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);\n",
        "        --shadow-xl: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);\n",
        "        --shadow-2xl: 0 25px 50px -12px rgba(0, 0, 0, 0.25);\n",
        "        --border-radius: 8px;\n",
        "        --border-radius-md: 12px;\n",
        "        --border-radius-lg: 16px;\n",
        "        --border-radius-xl: 20px;\n",
        "        --border-radius-2xl: 24px;\n",
        "    }\n",
        "\n",
        "    /* Reset and base styles */\n",
        "    * {\n",
        "        box-sizing: border-box;\n",
        "    }\n",
        "\n",
        "    /* Main container */\n",
        "    .main .block-container {\n",
        "        padding: 2rem 1rem;\n",
        "        max-width: 1200px;\n",
        "        margin: 0 auto;\n",
        "        background: var(--gray-50);\n",
        "        min-height: 100vh;\n",
        "    }\n",
        "\n",
        "    /* Header styling - Clean & Modern */\n",
        "    .main-header {\n",
        "        background: var(--light-color);\n",
        "        padding: 3rem 2rem;\n",
        "        border-radius: var(--border-radius-2xl);\n",
        "        margin-bottom: 2rem;\n",
        "        box-shadow: var(--shadow-lg);\n",
        "        text-align: center;\n",
        "        color: var(--dark-color);\n",
        "        position: relative;\n",
        "        overflow: hidden;\n",
        "        border: 1px solid var(--gray-200);\n",
        "    }\n",
        "\n",
        "    .main-header::before {\n",
        "        content: '';\n",
        "        position: absolute;\n",
        "        top: 0;\n",
        "        left: 0;\n",
        "        right: 0;\n",
        "        height: 4px;\n",
        "        background: var(--gradient-primary);\n",
        "    }\n",
        "\n",
        "    .main-header h1 {\n",
        "        font-size: 3rem;\n",
        "        font-weight: 700;\n",
        "        margin: 0;\n",
        "        color: var(--dark-color);\n",
        "        position: relative;\n",
        "        z-index: 1;\n",
        "        letter-spacing: -0.025em;\n",
        "    }\n",
        "\n",
        "    .main-header p {\n",
        "        font-size: 1.125rem;\n",
        "        margin: 1rem 0 0 0;\n",
        "        color: var(--gray-600);\n",
        "        position: relative;\n",
        "        z-index: 1;\n",
        "        font-weight: 400;\n",
        "    }\n",
        "\n",
        "    /* Hero section - Clean stats */\n",
        "    .hero-stats {\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "        gap: 1.5rem;\n",
        "        margin-top: 2rem;\n",
        "        position: relative;\n",
        "        z-index: 1;\n",
        "        flex-wrap: wrap;\n",
        "    }\n",
        "\n",
        "    .hero-stat {\n",
        "        text-align: center;\n",
        "        background: var(--gray-50);\n",
        "        padding: 1.5rem 1.25rem;\n",
        "        border-radius: var(--border-radius-lg);\n",
        "        border: 1px solid var(--gray-200);\n",
        "        min-width: 120px;\n",
        "        transition: all 0.2s ease;\n",
        "    }\n",
        "\n",
        "    .hero-stat:hover {\n",
        "        transform: translateY(-2px);\n",
        "        box-shadow: var(--shadow-md);\n",
        "        border-color: var(--primary-color);\n",
        "    }\n",
        "\n",
        "    .hero-stat .number {\n",
        "        font-size: 1.75rem;\n",
        "        font-weight: 700;\n",
        "        display: block;\n",
        "        color: var(--primary-color);\n",
        "        margin-bottom: 0.25rem;\n",
        "    }\n",
        "\n",
        "    .hero-stat .label {\n",
        "        font-size: 0.875rem;\n",
        "        color: var(--gray-600);\n",
        "        margin: 0;\n",
        "        font-weight: 500;\n",
        "    }\n",
        "\n",
        "    /* Sidebar styling - Clean & Minimal */\n",
        "    .css-1d391kg {\n",
        "        background: var(--light-color);\n",
        "        border-right: 1px solid var(--gray-200);\n",
        "    }\n",
        "\n",
        "    .sidebar .sidebar-content {\n",
        "        background: var(--light-color);\n",
        "        padding: 1.5rem 1rem;\n",
        "    }\n",
        "\n",
        "    .sidebar .sidebar-content .element-container {\n",
        "        margin-bottom: 1.5rem;\n",
        "    }\n",
        "\n",
        "    .sidebar h3 {\n",
        "        color: var(--dark-color);\n",
        "        font-size: 1rem;\n",
        "        font-weight: 600;\n",
        "        margin-bottom: 1rem;\n",
        "        padding-bottom: 0.5rem;\n",
        "        border-bottom: 1px solid var(--gray-200);\n",
        "        letter-spacing: 0.025em;\n",
        "    }\n",
        "\n",
        "    /* Card styling - Clean & Modern */\n",
        "    .metric-card {\n",
        "        background: var(--light-color);\n",
        "        padding: 1.25rem;\n",
        "        border-radius: var(--border-radius-lg);\n",
        "        box-shadow: var(--shadow-sm);\n",
        "        border: 1px solid var(--gray-200);\n",
        "        margin-bottom: 1rem;\n",
        "        transition: all 0.2s ease;\n",
        "        position: relative;\n",
        "        overflow: hidden;\n",
        "    }\n",
        "\n",
        "    .metric-card::before {\n",
        "        content: '';\n",
        "        position: absolute;\n",
        "        top: 0;\n",
        "        left: 0;\n",
        "        right: 0;\n",
        "        height: 2px;\n",
        "        background: var(--gradient-primary);\n",
        "    }\n",
        "\n",
        "    .metric-card:hover {\n",
        "        transform: translateY(-2px);\n",
        "        box-shadow: var(--shadow-md);\n",
        "        border-color: var(--primary-color);\n",
        "    }\n",
        "\n",
        "    .metric-card h3 {\n",
        "        color: var(--gray-600);\n",
        "        font-size: 0.75rem;\n",
        "        font-weight: 600;\n",
        "        margin: 0 0 0.5rem 0;\n",
        "        text-transform: uppercase;\n",
        "        letter-spacing: 0.05em;\n",
        "    }\n",
        "\n",
        "    .metric-card .value {\n",
        "        font-size: 1.5rem;\n",
        "        font-weight: 700;\n",
        "        color: var(--dark-color);\n",
        "        margin: 0;\n",
        "    }\n",
        "\n",
        "    /* Search card - Clean & Modern */\n",
        "    .search-card {\n",
        "        background: var(--light-color);\n",
        "        border-radius: var(--border-radius-xl);\n",
        "        padding: 2rem;\n",
        "        box-shadow: var(--shadow-sm);\n",
        "        border: 1px solid var(--gray-200);\n",
        "        margin-bottom: 2rem;\n",
        "        position: relative;\n",
        "        overflow: hidden;\n",
        "    }\n",
        "\n",
        "    .search-card::before {\n",
        "        content: '';\n",
        "        position: absolute;\n",
        "        top: 0;\n",
        "        left: 0;\n",
        "        right: 0;\n",
        "        height: 3px;\n",
        "        background: var(--gradient-primary);\n",
        "    }\n",
        "\n",
        "    /* Results grid */\n",
        "    .results-grid {\n",
        "        display: grid;\n",
        "        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n",
        "        gap: 1.5rem;\n",
        "        margin-top: 2rem;\n",
        "    }\n",
        "\n",
        "    /* Button styling - Clean & Modern */\n",
        "    .stButton > button {\n",
        "        background: var(--primary-color);\n",
        "        color: white;\n",
        "        border: none;\n",
        "        border-radius: var(--border-radius-md);\n",
        "        padding: 0.75rem 1.5rem;\n",
        "        font-weight: 600;\n",
        "        font-size: 0.875rem;\n",
        "        transition: all 0.2s ease;\n",
        "        box-shadow: var(--shadow-sm);\n",
        "        position: relative;\n",
        "        overflow: hidden;\n",
        "    }\n",
        "\n",
        "    .stButton > button:hover {\n",
        "        background: var(--primary-dark);\n",
        "        transform: translateY(-1px);\n",
        "        box-shadow: var(--shadow-md);\n",
        "    }\n",
        "\n",
        "    .stButton > button:active {\n",
        "        transform: translateY(0);\n",
        "        box-shadow: var(--shadow-sm);\n",
        "    }\n",
        "\n",
        "    /* Primary button variant */\n",
        "    .stButton > button[kind=\"primary\"] {\n",
        "        background: var(--primary-color);\n",
        "        box-shadow: var(--shadow-md);\n",
        "    }\n",
        "\n",
        "    .stButton > button[kind=\"primary\"]:hover {\n",
        "        background: var(--primary-dark);\n",
        "        box-shadow: var(--shadow-lg);\n",
        "    }\n",
        "\n",
        "    /* Search input styling - Clean & Modern */\n",
        "    .stTextInput > div > div > input {\n",
        "        border-radius: var(--border-radius-md);\n",
        "        border: 1px solid var(--gray-300);\n",
        "        padding: 0.875rem 1rem;\n",
        "        font-size: 1rem;\n",
        "        font-weight: 400;\n",
        "        transition: all 0.2s ease;\n",
        "        background: var(--light-color);\n",
        "        box-shadow: var(--shadow-xs);\n",
        "    }\n",
        "\n",
        "    .stTextInput > div > div > input:focus {\n",
        "        border-color: var(--primary-color);\n",
        "        box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.1), var(--shadow-xs);\n",
        "        background: var(--light-color);\n",
        "        outline: none;\n",
        "    }\n",
        "\n",
        "    .stTextInput > div > div > input::placeholder {\n",
        "        color: var(--gray-400);\n",
        "        font-weight: 400;\n",
        "    }\n",
        "\n",
        "    /* Popular search buttons - Clean & Modern */\n",
        "    .popular-search-btn {\n",
        "        background: var(--light-color);\n",
        "        border: 1px solid var(--gray-300);\n",
        "        border-radius: var(--border-radius);\n",
        "        padding: 0.5rem 0.875rem;\n",
        "        margin: 0.25rem;\n",
        "        font-size: 0.875rem;\n",
        "        font-weight: 500;\n",
        "        transition: all 0.2s ease;\n",
        "        display: inline-block;\n",
        "        text-decoration: none;\n",
        "        color: var(--gray-700);\n",
        "        box-shadow: var(--shadow-xs);\n",
        "    }\n",
        "\n",
        "    .popular-search-btn:hover {\n",
        "        border-color: var(--primary-color);\n",
        "        background: var(--primary-color);\n",
        "        color: white;\n",
        "        transform: translateY(-1px);\n",
        "        box-shadow: var(--shadow-sm);\n",
        "    }\n",
        "\n",
        "    /* Results styling - Clean & Modern */\n",
        "    .result-card {\n",
        "        background: var(--light-color);\n",
        "        border-radius: var(--border-radius-lg);\n",
        "        box-shadow: var(--shadow-sm);\n",
        "        overflow: hidden;\n",
        "        transition: all 0.2s ease;\n",
        "        border: 1px solid var(--gray-200);\n",
        "    }\n",
        "\n",
        "    .result-card:hover {\n",
        "        transform: translateY(-2px);\n",
        "        box-shadow: var(--shadow-md);\n",
        "        border-color: var(--primary-color);\n",
        "    }\n",
        "\n",
        "    .result-card img {\n",
        "        width: 100%;\n",
        "        height: 200px;\n",
        "        object-fit: cover;\n",
        "    }\n",
        "\n",
        "    .result-card .content {\n",
        "        padding: 1rem;\n",
        "    }\n",
        "\n",
        "    .result-card .similarity {\n",
        "        background: var(--primary-color);\n",
        "        color: white;\n",
        "        padding: 0.25rem 0.5rem;\n",
        "        border-radius: var(--border-radius);\n",
        "        font-size: 0.75rem;\n",
        "        font-weight: 600;\n",
        "        display: inline-block;\n",
        "        margin-bottom: 0.5rem;\n",
        "    }\n",
        "\n",
        "    .result-card .caption {\n",
        "        color: var(--gray-700);\n",
        "        font-size: 0.875rem;\n",
        "        line-height: 1.5;\n",
        "        margin: 0;\n",
        "    }\n",
        "\n",
        "    /* Status messages - Clean & Modern */\n",
        "    .stSuccess {\n",
        "        background: var(--success-color);\n",
        "        color: white;\n",
        "        padding: 1rem;\n",
        "        border-radius: var(--border-radius-md);\n",
        "        border: none;\n",
        "        box-shadow: var(--shadow-sm);\n",
        "    }\n",
        "\n",
        "    .stError {\n",
        "        background: var(--error-color);\n",
        "        color: white;\n",
        "        padding: 1rem;\n",
        "        border-radius: var(--border-radius-md);\n",
        "        border: none;\n",
        "        box-shadow: var(--shadow-sm);\n",
        "    }\n",
        "\n",
        "    .stWarning {\n",
        "        background: var(--warning-color);\n",
        "        color: white;\n",
        "        padding: 1rem;\n",
        "        border-radius: var(--border-radius-md);\n",
        "        border: none;\n",
        "        box-shadow: var(--shadow-sm);\n",
        "    }\n",
        "\n",
        "    /* Tabs styling - Clean & Modern */\n",
        "    .stTabs [data-baseweb=\"tab-list\"] {\n",
        "        gap: 0.25rem;\n",
        "        background: var(--gray-100);\n",
        "        padding: 0.25rem;\n",
        "        border-radius: var(--border-radius-lg);\n",
        "        margin-bottom: 2rem;\n",
        "        border: 1px solid var(--gray-200);\n",
        "    }\n",
        "\n",
        "    .stTabs [data-baseweb=\"tab\"] {\n",
        "        background: transparent;\n",
        "        border-radius: var(--border-radius-md);\n",
        "        border: none;\n",
        "        padding: 0.75rem 1.5rem;\n",
        "        font-weight: 600;\n",
        "        font-size: 0.875rem;\n",
        "        transition: all 0.2s ease;\n",
        "        color: var(--gray-600);\n",
        "        position: relative;\n",
        "        overflow: hidden;\n",
        "    }\n",
        "\n",
        "    .stTabs [data-baseweb=\"tab\"]:hover {\n",
        "        background: var(--gray-200);\n",
        "        color: var(--primary-color);\n",
        "    }\n",
        "\n",
        "    .stTabs [aria-selected=\"true\"] {\n",
        "        background: var(--primary-color);\n",
        "        color: white;\n",
        "        box-shadow: var(--shadow-sm);\n",
        "    }\n",
        "\n",
        "    /* Slider styling - Clean & Modern */\n",
        "    .stSlider > div > div > div > div {\n",
        "        background: var(--primary-color);\n",
        "    }\n",
        "\n",
        "    /* File uploader styling - Clean & Modern */\n",
        "    .stFileUploader > div > div > div {\n",
        "        border: 2px dashed var(--gray-300);\n",
        "        border-radius: var(--border-radius-lg);\n",
        "        padding: 2rem;\n",
        "        text-align: center;\n",
        "        transition: all 0.2s ease;\n",
        "        background: var(--gray-50);\n",
        "    }\n",
        "\n",
        "    .stFileUploader > div > div > div:hover {\n",
        "        border-color: var(--primary-color);\n",
        "        background: rgba(37, 99, 235, 0.05);\n",
        "    }\n",
        "\n",
        "    /* Responsive design - Clean & Modern */\n",
        "    @media (max-width: 768px) {\n",
        "        .main .block-container {\n",
        "            padding: 1rem 0.5rem;\n",
        "        }\n",
        "        \n",
        "        .main-header {\n",
        "            padding: 2rem 1rem;\n",
        "        }\n",
        "        \n",
        "        .main-header h1 {\n",
        "            font-size: 2rem;\n",
        "        }\n",
        "        \n",
        "        .main-header p {\n",
        "            font-size: 1rem;\n",
        "        }\n",
        "        \n",
        "        .hero-stats {\n",
        "            gap: 1rem;\n",
        "        }\n",
        "        \n",
        "        .hero-stat {\n",
        "            min-width: 100px;\n",
        "            padding: 1rem 0.75rem;\n",
        "        }\n",
        "        \n",
        "        .hero-stat .number {\n",
        "            font-size: 1.5rem;\n",
        "        }\n",
        "        \n",
        "        .hero-stat .label {\n",
        "            font-size: 0.75rem;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    @media (max-width: 480px) {\n",
        "        .main-header h1 {\n",
        "            font-size: 1.75rem;\n",
        "        }\n",
        "        \n",
        "        .hero-stats {\n",
        "            flex-direction: column;\n",
        "            align-items: center;\n",
        "        }\n",
        "        \n",
        "        .hero-stat {\n",
        "            width: 100%;\n",
        "            max-width: 200px;\n",
        "        }\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Main Streamlit app\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"üîç Multimodal Search Engine\",\n",
        "        page_icon=\"üîç\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\"\n",
        "    )\n",
        "\n",
        "    # Clear cache if needed (for debugging)\n",
        "    if st.sidebar.button(\"üóëÔ∏è Clear Cache\"):\n",
        "        st.cache_data.clear()\n",
        "        st.cache_resource.clear()\n",
        "        st.rerun()\n",
        "\n",
        "    # Load custom CSS\n",
        "    load_css()\n",
        "\n",
        "    # Modern header with hero stats\n",
        "    st.markdown(\"\"\"\n",
        "    <div class=\"main-header\">\n",
        "        <h1>üîç Multimodal Search Engine</h1>\n",
        "        <p>Powered by OpenAI CLIP ‚Ä¢ Find images with text or text with images</p>\n",
        "        <div class=\"hero-stats\">\n",
        "            <div class=\"hero-stat\">\n",
        "                <span class=\"number\">\"\"\" + str(model_info.get('num_images', 'Unknown')) + \"\"\"</span>\n",
        "                <span class=\"label\">Images</span>\n",
        "            </div>\n",
        "            <div class=\"hero-stat\">\n",
        "                <span class=\"number\">\"\"\" + str(model_info.get('embedding_dim', 'Unknown')) + \"\"\"D</span>\n",
        "                <span class=\"label\">Embeddings</span>\n",
        "            </div>\n",
        "            <div class=\"hero-stat\">\n",
        "                <span class=\"number\">\"\"\" + str(model_info.get('dataset', 'Unknown')) + \"\"\"</span>\n",
        "                <span class=\"label\">Dataset</span>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # Modern sidebar\n",
        "    st.sidebar.markdown(\"### ‚öôÔ∏è Search Configuration\")\n",
        "    \n",
        "    # Search type selection with modern styling\n",
        "    search_type = st.sidebar.selectbox(\n",
        "        \"üîç Search Type\",\n",
        "        [\"Text-to-Image Search\", \"Image-to-Text Search\"],\n",
        "        help=\"Choose how you want to search\"\n",
        "    )\n",
        "\n",
        "    # Number of results with modern slider\n",
        "    st.sidebar.markdown(\"### üìä Results\")\n",
        "    top_k = st.sidebar.slider(\n",
        "        \"Number of results\",\n",
        "        min_value=1,\n",
        "        max_value=20,\n",
        "        value=5,\n",
        "        help=\"Number of top results to display\"\n",
        "    )\n",
        "\n",
        "    # Popular searches with modern grid\n",
        "    st.sidebar.markdown(\"### üî• Popular Searches\")\n",
        "    st.sidebar.markdown(\"*Click any suggestion to search instantly*\")\n",
        "\n",
        "    popular_searches = [\n",
        "        \"dog playing\", \"children smiling\", \"red car\", \"food cooking\",\n",
        "        \"person running\", \"cat sleeping\", \"blue sky\", \"water beach\",\n",
        "        \"house building\", \"tree nature\", \"person walking\", \"animal pet\"\n",
        "    ]\n",
        "\n",
        "    # Create a grid of popular search buttons\n",
        "    cols = st.sidebar.columns(2)\n",
        "    for i, search in enumerate(popular_searches):\n",
        "        with cols[i % 2]:\n",
        "            if st.button(f\"üîç {search}\", key=f\"popular_{i}\", help=f\"Search for '{search}'\"):\n",
        "                st.session_state.popular_search = search\n",
        "                st.session_state.auto_search = True\n",
        "\n",
        "    # Dataset information with modern cards\n",
        "    st.sidebar.markdown(\"### üìä Dataset Information\")\n",
        "    \n",
        "    # Get values and format properly\n",
        "    num_images = model_info.get('num_images', 'Unknown')\n",
        "    num_embeddings = model_info.get('total_embeddings', model_info.get('num_samples', 'Unknown'))\n",
        "    embedding_dim = model_info.get('embedding_dim', 'Unknown')\n",
        "    model_name = model_info.get('model_name', 'Unknown')\n",
        "    dataset = model_info.get('dataset', 'Unknown')\n",
        "    processing_date = model_info.get('processing_date', datetime.now().strftime('%Y-%m-%d'))\n",
        "\n",
        "    # Format numbers properly\n",
        "    images_text = f\"{num_images:,}\" if isinstance(num_images, int) else str(num_images)\n",
        "    embeddings_text = f\"{num_embeddings:,}\" if isinstance(num_embeddings, int) else str(num_embeddings)\n",
        "    model_display = model_name.split('/')[-1] if '/' in model_name else model_name\n",
        "\n",
        "    # Display metrics in modern cards\n",
        "    st.sidebar.markdown(f\"\"\"\n",
        "    <div class=\"metric-card\">\n",
        "        <h3>üì∏ Total Images</h3>\n",
        "        <div class=\"value\">{images_text}</div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "    \n",
        "    st.sidebar.markdown(f\"\"\"\n",
        "    <div class=\"metric-card\">\n",
        "        <h3>üî¢ Total Embeddings</h3>\n",
        "        <div class=\"value\">{embeddings_text}</div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "    \n",
        "    st.sidebar.markdown(f\"\"\"\n",
        "    <div class=\"metric-card\">\n",
        "        <h3>üìê Embedding Dimension</h3>\n",
        "        <div class=\"value\">{embedding_dim}D</div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "    \n",
        "    st.sidebar.markdown(f\"\"\"\n",
        "    <div class=\"metric-card\">\n",
        "        <h3>ü§ñ Model</h3>\n",
        "        <div class=\"value\">{model_display}</div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "    \n",
        "    st.sidebar.markdown(f\"\"\"\n",
        "    <div class=\"metric-card\">\n",
        "        <h3>üìÅ Dataset</h3>\n",
        "        <div class=\"value\">{dataset}</div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "    \n",
        "    st.sidebar.markdown(f\"\"\"\n",
        "    <div class=\"metric-card\">\n",
        "        <h3>üìÖ Processing Date</h3>\n",
        "        <div class=\"value\">{processing_date}</div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # Check if this is a demo dataset\n",
        "    num_images = model_info.get('num_images', len(metadata))\n",
        "    if isinstance(num_images, int) and num_images < 1000:\n",
        "        st.warning(f\"‚ö†Ô∏è **Demo Mode**: You're using a small subset ({num_images:,} images) of the full Flickr8k dataset. For production use, run the full dataset processing in Part 1 to get all 8,091 images.\")\n",
        "\n",
        "    # Main content area with modern tabs\n",
        "    tab1, tab2 = st.tabs([\"üî§ Text-to-Image Search\", \"üñºÔ∏è Image-to-Text Search\"])\n",
        "    \n",
        "    # Add clarification about the tabs\n",
        "    st.info(\"üí° **Tip**: Use the **Text-to-Image** tab to search for images using text descriptions. Use the **Image-to-Text** tab to upload an image and find similar text descriptions.\")\n",
        "    \n",
        "    with tab1:\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"search-card\">\n",
        "            <h2 style=\"margin: 0 0 1rem 0; color: var(--dark-color); font-size: 1.5rem; font-weight: 700;\">üî§ Text-to-Image Search</h2>\n",
        "            <p style=\"margin: 0 0 2rem 0; color: var(--gray-600); font-size: 1rem;\">Describe what you're looking for and discover relevant images from the dataset</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Search suggestions with modern cards\n",
        "        col1, col2 = st.columns([1, 1])\n",
        "        \n",
        "        with col1:\n",
        "            st.markdown(\"\"\"\n",
        "            <div style=\"background: var(--gray-50); padding: 1.5rem; border-radius: var(--border-radius-lg); border-left: 3px solid var(--primary-color); margin-bottom: 1rem;\">\n",
        "                <h4 style=\"margin: 0 0 1rem 0; color: var(--dark-color); font-size: 1rem; font-weight: 600;\">üí° Search Tips</h4>\n",
        "                <div style=\"color: var(--gray-600); line-height: 1.6; font-size: 0.875rem;\">\n",
        "                    <strong>Try searching for:</strong><br>\n",
        "                    ‚Ä¢ <strong>Animals:</strong> 'dog', 'cat', 'bird', 'horse'<br>\n",
        "                    ‚Ä¢ <strong>Activities:</strong> 'playing', 'running', 'cooking'<br>\n",
        "                    ‚Ä¢ <strong>Objects:</strong> 'car', 'house', 'food'<br>\n",
        "                    ‚Ä¢ <strong>Emotions:</strong> 'smiling', 'happy', 'sad'<br>\n",
        "                    ‚Ä¢ <strong>Scenes:</strong> 'beach', 'park', 'kitchen'\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "        \n",
        "        with col2:\n",
        "            st.markdown(\"\"\"\n",
        "            <div style=\"background: var(--gray-50); padding: 1.5rem; border-radius: var(--border-radius-lg); border-left: 3px solid var(--success-color); margin-bottom: 1rem;\">\n",
        "                <h4 style=\"margin: 0 0 1rem 0; color: var(--dark-color); font-size: 1rem; font-weight: 600;\">üéØ Quick Examples</h4>\n",
        "                <div style=\"color: var(--gray-600); line-height: 1.6; font-size: 0.875rem;\">\n",
        "                    Click any example to search instantly:\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "            \n",
        "            # Example buttons in a grid\n",
        "            example_cols = st.columns(2)\n",
        "            with example_cols[0]:\n",
        "                if st.button(\"üêï A dog playing\", key=\"example1\", help=\"Search for 'a dog playing'\"):\n",
        "                    st.session_state.example_query = \"a dog playing\"\n",
        "                    st.session_state.auto_search = True\n",
        "                if st.button(\"üë∂ Children smiling\", key=\"example2\", help=\"Search for 'children smiling'\"):\n",
        "                    st.session_state.example_query = \"children smiling\"\n",
        "                    st.session_state.auto_search = True\n",
        "            \n",
        "            with example_cols[1]:\n",
        "                if st.button(\"üöó Red car\", key=\"example3\", help=\"Search for 'red car'\"):\n",
        "                    st.session_state.example_query = \"red car\"\n",
        "                    st.session_state.auto_search = True\n",
        "                if st.button(\"üçï Food cooking\", key=\"example4\", help=\"Search for 'food cooking'\"):\n",
        "                    st.session_state.example_query = \"food cooking\"\n",
        "                    st.session_state.auto_search = True\n",
        "\n",
        "        # Text input with better placeholder\n",
        "        query_text = st.text_input(\n",
        "            \"üîç Enter your search query:\",\n",
        "            placeholder=\"Describe what you're looking for... (e.g., 'a dog playing in the park', 'children smiling', 'red car on street')\",\n",
        "            help=\"üí° Be specific! Try describing objects, actions, colors, or emotions. The more descriptive, the better the results!\",\n",
        "            value=st.session_state.get('example_query', st.session_state.get('popular_search', '')),\n",
        "            key=\"search_input\"\n",
        "        )\n",
        "\n",
        "        # Clear example queries after use\n",
        "        if 'example_query' in st.session_state:\n",
        "            del st.session_state.example_query\n",
        "        if 'popular_search' in st.session_state:\n",
        "            del st.session_state.popular_search\n",
        "\n",
        "        # Check if we should auto-search (from popular searches or example queries)\n",
        "        should_search = st.session_state.get('auto_search', False)\n",
        "        if should_search:\n",
        "            st.session_state.auto_search = False  # Reset the flag\n",
        "            # Use example query if available, otherwise use popular search\n",
        "            query_text = st.session_state.get('example_query', st.session_state.get('popular_search', query_text))\n",
        "\n",
        "        if st.button(\"üîç Search Images\", type=\"primary\") or should_search:\n",
        "            if query_text:\n",
        "                with st.spinner(\"Searching for images...\"):\n",
        "                    results = text_to_image_search(query_text, top_k)\n",
        "\n",
        "                if results:\n",
        "                    st.success(f\"Found {len(results)} results for: '{query_text}'\")\n",
        "\n",
        "                    # Display results in columns\n",
        "                    cols = st.columns(min(3, len(results)))\n",
        "                    for i, result in enumerate(results):\n",
        "                        with cols[i % 3]:\n",
        "                            try:\n",
        "                                image_path = result['image_path']\n",
        "                                # Fix path - remove ../ if present\n",
        "                                if image_path.startswith('../'):\n",
        "                                    image_path = image_path[3:]  # Remove ../\n",
        "\n",
        "                                if os.path.exists(image_path):\n",
        "                                    image = Image.open(image_path)\n",
        "                                    st.image(image, caption=f\"Similarity: {result['similarity']:.3f}\", use_container_width=True)\n",
        "\n",
        "                                    # Display details\n",
        "                                    st.markdown(f\"**Image ID:** {result['image_id']}\")\n",
        "                                    st.markdown(f\"**Caption:** {result['caption']}\")\n",
        "                                    st.markdown(f\"**Similarity:** {result['similarity']:.3f}\")\n",
        "                                else:\n",
        "                                    st.error(f\"Image not found: {image_path}\")\n",
        "                            except Exception as e:\n",
        "                                st.error(f\"Error loading image: {e}\")\n",
        "                    else:\n",
        "                        st.warning(\"No results found. Try a different search query.\")\n",
        "            else:\n",
        "                st.warning(\"Please enter a search query.\")\n",
        "\n",
        "    with tab2:\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"search-card\">\n",
        "            <h2 style=\"margin: 0 0 1rem 0; color: var(--dark-color); font-size: 1.5rem; font-weight: 700;\">üñºÔ∏è Image-to-Text Search</h2>\n",
        "            <p style=\"margin: 0 0 2rem 0; color: var(--gray-600); font-size: 1rem;\">Upload an image to find similar text descriptions from the dataset</p>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Upload guidance\n",
        "        st.markdown(\"#### üìã Upload Guidelines\")\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(\"\"\"\n",
        "            <div style=\"background: var(--gray-50); padding: 1.25rem; border-radius: var(--border-radius-lg); border-left: 3px solid var(--primary-color);\">\n",
        "                <h4 style=\"margin: 0 0 0.75rem 0; color: var(--dark-color); font-size: 0.875rem; font-weight: 600;\">Supported formats:</h4>\n",
        "                <div style=\"color: var(--gray-600); font-size: 0.875rem; line-height: 1.5;\">\n",
        "                    ‚Ä¢ JPG, JPEG<br>\n",
        "                    ‚Ä¢ PNG<br>\n",
        "                    ‚Ä¢ BMP, GIF\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        with col2:\n",
        "            st.markdown(\"\"\"\n",
        "            <div style=\"background: var(--gray-50); padding: 1.25rem; border-radius: var(--border-radius-lg); border-left: 3px solid var(--success-color);\">\n",
        "                <h4 style=\"margin: 0 0 0.75rem 0; color: var(--dark-color); font-size: 0.875rem; font-weight: 600;\">Best results with:</h4>\n",
        "                <div style=\"color: var(--gray-600); font-size: 0.875rem; line-height: 1.5;\">\n",
        "                    ‚Ä¢ Clear, well-lit images<br>\n",
        "                    ‚Ä¢ Single main subject<br>\n",
        "                    ‚Ä¢ Good contrast\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Image upload\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"üìÅ Choose an image file:\",\n",
        "            type=['jpg', 'jpeg', 'png', 'bmp', 'gif'],\n",
        "            help=\"üí° Upload a clear image with a main subject for best search results!\",\n",
        "            label_visibility=\"collapsed\"\n",
        "        )\n",
        "        \n",
        "        # Add some guidance\n",
        "        if not uploaded_file:\n",
        "            st.info(\"üëÜ **Upload an image above** to find similar text descriptions from the dataset\")\n",
        "\n",
        "        if uploaded_file is not None:\n",
        "            try:\n",
        "                # Debug information\n",
        "                st.write(f\"üìÅ File name: {uploaded_file.name}\")\n",
        "                st.write(f\"üìè File size: {uploaded_file.size} bytes\")\n",
        "                st.write(f\"üîç File type: {uploaded_file.type}\")\n",
        "                \n",
        "                # Reset file pointer to beginning\n",
        "                uploaded_file.seek(0)\n",
        "                \n",
        "                # Try using BytesIO with proper handling\n",
        "                file_bytes = uploaded_file.read()\n",
        "                st.write(f\"üìä File bytes length: {len(file_bytes)}\")\n",
        "                \n",
        "                # Check if file has content\n",
        "                if len(file_bytes) == 0:\n",
        "                    st.error(\"‚ùå File is empty!\")\n",
        "                    return\n",
        "                \n",
        "                # Try to create image from bytes using a more robust approach\n",
        "                try:\n",
        "                    # Create BytesIO object\n",
        "                    image_io = io.BytesIO(file_bytes)\n",
        "                    image_io.seek(0)\n",
        "                    \n",
        "                    # Try to determine format from file extension\n",
        "                    file_extension = os.path.splitext(uploaded_file.name)[1].lower()\n",
        "                    st.write(f\"üîç Detected file extension: {file_extension}\")\n",
        "                    \n",
        "                    # Try to open with PIL - let it auto-detect the format\n",
        "                    uploaded_image = Image.open(image_io)\n",
        "                    \n",
        "                    # Load the image data\n",
        "                    uploaded_image.load()\n",
        "                    \n",
        "                    # Convert to RGB if necessary\n",
        "                    if uploaded_image.mode != 'RGB':\n",
        "                        uploaded_image = uploaded_image.convert('RGB')\n",
        "                    \n",
        "                    st.success(\"‚úÖ Image loaded successfully!\")\n",
        "                    \n",
        "                except Exception as img_error:\n",
        "                    st.error(f\"‚ùå Error loading image: {str(img_error)}\")\n",
        "                    \n",
        "                    # Try alternative approach - save to temporary file\n",
        "                    st.write(\"üîÑ Trying temporary file approach...\")\n",
        "                    \n",
        "                    try:\n",
        "                        import tempfile\n",
        "                        \n",
        "                        # Create temporary file with proper extension\n",
        "                        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:\n",
        "                            tmp_file.write(file_bytes)\n",
        "                            tmp_file_path = tmp_file.name\n",
        "                        \n",
        "                        st.write(f\"üìÅ Created temp file: {tmp_file_path}\")\n",
        "                        \n",
        "                        # Load from temporary file\n",
        "                        uploaded_image = Image.open(tmp_file_path)\n",
        "                        uploaded_image.load()\n",
        "                        \n",
        "                        # Convert to RGB if necessary\n",
        "                        if uploaded_image.mode != 'RGB':\n",
        "                            uploaded_image = uploaded_image.convert('RGB')\n",
        "                        \n",
        "                        # Clean up temporary file\n",
        "                        os.unlink(tmp_file_path)\n",
        "                        \n",
        "                        st.success(\"‚úÖ Image loaded with temporary file method!\")\n",
        "                        \n",
        "                    except Exception as temp_error:\n",
        "                        st.error(f\"‚ùå Temporary file method failed: {str(temp_error)}\")\n",
        "                        \n",
        "                        # Final fallback - try with cv2 if available\n",
        "                        st.write(\"üîÑ Trying OpenCV fallback...\")\n",
        "                        try:\n",
        "                            import cv2\n",
        "                            import numpy as np\n",
        "                            \n",
        "                            # Convert bytes to numpy array\n",
        "                            nparr = np.frombuffer(file_bytes, np.uint8)\n",
        "                            \n",
        "                            # Decode image with OpenCV\n",
        "                            cv_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "                            \n",
        "                            if cv_image is not None:\n",
        "                                # Convert BGR to RGB\n",
        "                                cv_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
        "                                \n",
        "                                # Convert to PIL Image\n",
        "                                uploaded_image = Image.fromarray(cv_image)\n",
        "                                \n",
        "                                st.success(\"‚úÖ Image loaded with OpenCV fallback!\")\n",
        "                            else:\n",
        "                                raise Exception(\"OpenCV could not decode the image\")\n",
        "                                \n",
        "                        except ImportError:\n",
        "                            st.error(\"‚ùå OpenCV not available for fallback\")\n",
        "                            st.warning(\"The uploaded file might be corrupted or in an unsupported format.\")\n",
        "                            return\n",
        "                        except Exception as cv_error:\n",
        "                            st.error(f\"‚ùå OpenCV fallback failed: {str(cv_error)}\")\n",
        "                            st.warning(\"The uploaded file might be corrupted or in an unsupported format.\")\n",
        "                            return\n",
        "                \n",
        "                # Display the image\n",
        "                st.image(uploaded_image, caption=\"Uploaded Image\", use_container_width=True)\n",
        "\n",
        "                if st.button(\"üîç Search Descriptions\", type=\"primary\"):\n",
        "                    with st.spinner(\"Searching for similar descriptions...\"):\n",
        "                        try:\n",
        "                            # Use the image we already loaded\n",
        "                            results = image_to_text_search(uploaded_image, top_k)\n",
        "                        except Exception as search_error:\n",
        "                            st.error(f\"‚ùå Error during search: {str(search_error)}\")\n",
        "                            results = []\n",
        "\n",
        "                    if results:\n",
        "                        st.success(f\"Found {len(results)} similar descriptions\")\n",
        "\n",
        "                        # Display results in columns\n",
        "                        cols = st.columns(min(3, len(results)))\n",
        "                        for i, result in enumerate(results):\n",
        "                            with cols[i % 3]:\n",
        "                                try:\n",
        "                                    image_path = result['image_path']\n",
        "                                    # Fix path - remove ../ if present\n",
        "                                    if image_path.startswith('../'):\n",
        "                                        image_path = image_path[3:]  # Remove ../\n",
        "\n",
        "                                    if os.path.exists(image_path):\n",
        "                                        original_image = Image.open(image_path)\n",
        "                                        st.image(original_image, caption=\"Original Image\", use_container_width=True)\n",
        "                                    else:\n",
        "                                        st.error(f\"Original image not found: {image_path}\")\n",
        "                                except Exception as e:\n",
        "                                    st.error(f\"Error loading original image: {e}\")\n",
        "\n",
        "                                # Display details\n",
        "                                st.markdown(f\"**Image ID:** {result['image_id']}\")\n",
        "                                st.markdown(f\"**Caption:** {result['caption']}\")\n",
        "                                st.markdown(f\"**Similarity:** {result['similarity']:.3f}\")\n",
        "                    else:\n",
        "                        st.warning(\"No results found. Try a different image.\")\n",
        "                        \n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå Error loading image: {str(e)}\")\n",
        "                st.warning(\"Please make sure you're uploading a valid image file (JPG, PNG, BMP, GIF)\")\n",
        "                uploaded_file = None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "'''\n",
        "\n",
        "# Write Streamlit app to file\n",
        "with open('../streamlit_app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"‚úÖ Streamlit app created successfully!\")\n",
        "print(\"üìÅ File saved as: ../streamlit_app.py\")\n",
        "print(\"üöÄ To run: streamlit run ../streamlit_app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Standalone Gradio App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Gradio app created successfully!\n",
            "üìÅ File saved as: ../gradio_app.py\n",
            "üöÄ To run: python ../gradio_app.py\n"
          ]
        }
      ],
      "source": [
        "# Create the complete Gradio app code\n",
        "gradio_code = '''\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CLIP model\n",
        "def load_clip_model():\n",
        "    \"\"\"Load CLIP model and processor\"\"\"\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    return model, processor\n",
        "\n",
        "# Load embeddings data\n",
        "def load_embeddings_data():\n",
        "    \"\"\"Load pre-computed embeddings and metadata\"\"\"\n",
        "    # Load embeddings\n",
        "    image_embeddings = np.load('embeddings/image_embeddings.npy')\n",
        "    text_embeddings = np.load('embeddings/text_embeddings.npy')\n",
        "\n",
        "    # Load metadata\n",
        "    metadata = pd.read_csv('embeddings/metadata.csv')\n",
        "\n",
        "    # Load model info\n",
        "    with open('embeddings/model_info.json', 'r') as f:\n",
        "        model_info = json.load(f)\n",
        "\n",
        "    return image_embeddings, text_embeddings, metadata, model_info\n",
        "\n",
        "# Load model and data\n",
        "model, processor = load_clip_model()\n",
        "image_embeddings, text_embeddings, metadata, model_info = load_embeddings_data()\n",
        "\n",
        "# Text-to-Image Search Function\n",
        "def text_to_image_search(query_text, top_k=5):\n",
        "    \"\"\"Search for images based on text query\"\"\"\n",
        "    # Generate embedding for text query\n",
        "    inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_text_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Calculate similarities with all image embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), image_embeddings)[0]\n",
        "\n",
        "    # Get top-k most similar images\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Image-to-Text Search Function\n",
        "def image_to_text_search(uploaded_image, top_k=5):\n",
        "    \"\"\"Search for text descriptions based on uploaded image\"\"\"\n",
        "    # Generate embedding for uploaded image\n",
        "    inputs = processor(images=uploaded_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_image_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Calculate similarities with all text embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), text_embeddings)[0]\n",
        "\n",
        "    # Get top-k most similar text descriptions\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Text-to-Image Search Interface\n",
        "def search_images(query, num_results):\n",
        "    \"\"\"Gradio interface for text-to-image search\"\"\"\n",
        "    if not query.strip():\n",
        "        return [], \"Please enter a search query.\"\n",
        "\n",
        "    try:\n",
        "        results = text_to_image_search(query, num_results)\n",
        "\n",
        "        if not results:\n",
        "            return [], \"No results found. Try a different search query.\"\n",
        "\n",
        "        # Prepare gallery items as tuples (image_path, caption)\n",
        "        gallery_items = []\n",
        "\n",
        "        for result in results:\n",
        "            image_path = result['image_path']\n",
        "            # Fix path - remove ../ if present\n",
        "            if image_path.startswith('../'):\n",
        "                image_path = image_path[3:]  # Remove ../\n",
        "\n",
        "            if os.path.exists(image_path):\n",
        "                caption = f\"üéØ Similarity: {result['similarity']:.3f}\\nüìù {result['caption']}\"\n",
        "                gallery_items.append((image_path, caption))\n",
        "            else:\n",
        "                # For missing images, we can't add them to the gallery\n",
        "                pass\n",
        "\n",
        "        return gallery_items, f\"Found {len(gallery_items)} results for: '{query}'\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return [], f\"Error during search: {str(e)}\"\n",
        "\n",
        "# Image-to-Text Search Interface\n",
        "def search_descriptions(image, num_results):\n",
        "    \"\"\"Gradio interface for image-to-text search\"\"\"\n",
        "    if image is None:\n",
        "        return [], \"Please upload an image.\"\n",
        "\n",
        "    try:\n",
        "        results = image_to_text_search(image, num_results)\n",
        "\n",
        "        if not results:\n",
        "            return [], \"No results found. Try a different image.\"\n",
        "\n",
        "        # Prepare gallery items as tuples (image_path, caption)\n",
        "        gallery_items = []\n",
        "\n",
        "        for result in results:\n",
        "            image_path = result['image_path']\n",
        "            # Fix path - remove ../ if present\n",
        "            if image_path.startswith('../'):\n",
        "                image_path = image_path[3:]  # Remove ../\n",
        "\n",
        "            if os.path.exists(image_path):\n",
        "                caption = f\"üéØ Similarity: {result['similarity']:.3f}\\nüìù {result['caption']}\"\n",
        "                gallery_items.append((image_path, caption))\n",
        "            else:\n",
        "                # For missing images, we can't add them to the gallery\n",
        "                pass\n",
        "\n",
        "        return gallery_items, f\"Found {len(gallery_items)} similar descriptions\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return [], f\"Error during search: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_gradio_app():\n",
        "    \"\"\"Create the Gradio web application\"\"\"\n",
        "\n",
        "    # Project description\n",
        "    description = \"\"\"\n",
        "    # üîç Multimodal Search Engine\n",
        "\n",
        "    A powerful search engine that can find images using text descriptions and find text descriptions using images.\n",
        "\n",
        "    **Technology Stack:**\n",
        "    - **Model**: OpenAI CLIP (Contrastive Language-Image Pre-training)\n",
        "    - **Framework**: Gradio for web interface\n",
        "    - **Dataset**: Flickr8k (8,091 images with captions)\n",
        "    - **Embeddings**: 512-dimensional vector representations\n",
        "    - **Similarity**: Cosine similarity for matching\n",
        "\n",
        "    **Features:**\n",
        "    - Text-to-Image Search: Describe what you're looking for\n",
        "    - Image-to-Text Search: Upload an image to find similar descriptions\n",
        "    - Real-time similarity scoring\n",
        "    - Interactive web interface\n",
        "    \"\"\"\n",
        "\n",
        "    # Popular search suggestions\n",
        "    popular_searches = [\n",
        "        \"dog playing\", \"children smiling\", \"red car\", \"food cooking\",\n",
        "        \"person running\", \"cat sleeping\", \"blue sky\", \"water beach\"\n",
        "    ]\n",
        "\n",
        "    with gr.Blocks(\n",
        "        title=\"üîç Multimodal Search Engine\", \n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\"\"\"\n",
        "        .gradio-container {\n",
        "            max-width: 1400px !important;\n",
        "            margin: 0 auto !important;\n",
        "            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif !important;\n",
        "            background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%) !important;\n",
        "            min-height: 100vh !important;\n",
        "        }\n",
        "        \n",
        "        /* Modern header styling */\n",
        "        .gradio-container h1 {\n",
        "            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 50%, #ec4899 100%);\n",
        "            -webkit-background-clip: text;\n",
        "            -webkit-text-fill-color: transparent;\n",
        "            background-clip: text;\n",
        "            font-size: 3rem !important;\n",
        "            font-weight: 800 !important;\n",
        "            text-align: center !important;\n",
        "            margin: 2rem 0 !important;\n",
        "            text-shadow: 0 2px 4px rgba(0, 0, 0, 0.1) !important;\n",
        "        }\n",
        "        \n",
        "        /* Card styling */\n",
        "        .card {\n",
        "            background: white !important;\n",
        "            border-radius: 20px !important;\n",
        "            padding: 2rem !important;\n",
        "            box-shadow: 0 10px 25px -5px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05) !important;\n",
        "            border: 1px solid rgba(255, 255, 255, 0.2) !important;\n",
        "            backdrop-filter: blur(10px) !important;\n",
        "            margin: 1rem 0 !important;\n",
        "        }\n",
        "        \n",
        "        /* Button styling */\n",
        "        .btn {\n",
        "            border-radius: 16px !important;\n",
        "            font-weight: 700 !important;\n",
        "            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1) !important;\n",
        "            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06) !important;\n",
        "            border: none !important;\n",
        "            padding: 0.75rem 1.5rem !important;\n",
        "            font-size: 0.875rem !important;\n",
        "            text-transform: uppercase !important;\n",
        "            letter-spacing: 0.05em !important;\n",
        "        }\n",
        "        \n",
        "        .btn:hover {\n",
        "            transform: translateY(-2px) !important;\n",
        "            box-shadow: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04) !important;\n",
        "        }\n",
        "        \n",
        "        .btn-primary {\n",
        "            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%) !important;\n",
        "            color: white !important;\n",
        "        }\n",
        "        \n",
        "        /* Input styling */\n",
        "        .input {\n",
        "            border-radius: 16px !important;\n",
        "            border: 2px solid #e2e8f0 !important;\n",
        "            padding: 1rem 1.25rem !important;\n",
        "            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1) !important;\n",
        "            background: rgba(255, 255, 255, 0.8) !important;\n",
        "            backdrop-filter: blur(10px) !important;\n",
        "            font-size: 1rem !important;\n",
        "        }\n",
        "        \n",
        "        .input:focus {\n",
        "            border-color: #6366f1 !important;\n",
        "            box-shadow: 0 0 0 4px rgba(99, 102, 241, 0.1) !important;\n",
        "            background: white !important;\n",
        "        }\n",
        "        \n",
        "        /* Gallery styling */\n",
        "        .gallery {\n",
        "            border-radius: 20px !important;\n",
        "            overflow: hidden !important;\n",
        "            box-shadow: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04) !important;\n",
        "            background: white !important;\n",
        "            padding: 1rem !important;\n",
        "        }\n",
        "        \n",
        "        .gallery img {\n",
        "            border-radius: 16px !important;\n",
        "            transition: transform 0.3s ease !important;\n",
        "        }\n",
        "        \n",
        "        .gallery img:hover {\n",
        "            transform: scale(1.02) !important;\n",
        "        }\n",
        "        \n",
        "        /* Tab styling */\n",
        "        .tab-nav {\n",
        "            background: rgba(255, 255, 255, 0.8) !important;\n",
        "            backdrop-filter: blur(10px) !important;\n",
        "            border-radius: 20px !important;\n",
        "            padding: 0.5rem !important;\n",
        "            margin: 2rem 0 !important;\n",
        "            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1) !important;\n",
        "        }\n",
        "        \n",
        "        .tab-nav button {\n",
        "            border-radius: 12px !important;\n",
        "            font-weight: 700 !important;\n",
        "            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1) !important;\n",
        "            padding: 0.75rem 1.5rem !important;\n",
        "            margin: 0.25rem !important;\n",
        "        }\n",
        "        \n",
        "        .tab-nav button.selected {\n",
        "            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%) !important;\n",
        "            color: white !important;\n",
        "            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1) !important;\n",
        "        }\n",
        "        \n",
        "        /* Status messages */\n",
        "        .status {\n",
        "            background: linear-gradient(135deg, #10b981 0%, #059669 100%) !important;\n",
        "            color: white !important;\n",
        "            padding: 1rem 1.5rem !important;\n",
        "            border-radius: 12px !important;\n",
        "            font-weight: 600 !important;\n",
        "            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1) !important;\n",
        "        }\n",
        "        \n",
        "        /* Dataset info cards */\n",
        "        .dataset-card {\n",
        "            background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%) !important;\n",
        "            border-radius: 16px !important;\n",
        "            padding: 1.5rem !important;\n",
        "            margin: 0.5rem 0 !important;\n",
        "            border: 1px solid rgba(255, 255, 255, 0.2) !important;\n",
        "            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1) !important;\n",
        "        }\n",
        "        \n",
        "        /* Responsive design */\n",
        "        @media (max-width: 768px) {\n",
        "            .gradio-container {\n",
        "                padding: 1rem !important;\n",
        "            }\n",
        "            \n",
        "            .gradio-container h1 {\n",
        "                font-size: 2.5rem !important;\n",
        "            }\n",
        "            \n",
        "            .card {\n",
        "                padding: 1.5rem !important;\n",
        "                margin: 0.5rem 0 !important;\n",
        "            }\n",
        "        }\n",
        "        \"\"\"\n",
        "    ) as app:\n",
        "        gr.Markdown(description)\n",
        "\n",
        "        # Dataset information\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Get values and format properly\n",
        "                num_images = model_info.get('num_images', 'Unknown')\n",
        "                num_embeddings = model_info.get('total_embeddings', model_info.get('num_samples', 'Unknown'))\n",
        "                embedding_dim = model_info.get('embedding_dim', 'Unknown')\n",
        "                model_name = model_info.get('model_name', 'Unknown')\n",
        "                dataset = model_info.get('dataset', 'Unknown')\n",
        "                processing_date = model_info.get('processing_date', 'Unknown')\n",
        "                \n",
        "                # Format numbers properly\n",
        "                images_text = f\"{num_images:,}\" if isinstance(num_images, int) else str(num_images)\n",
        "                embeddings_text = f\"{num_embeddings:,}\" if isinstance(num_embeddings, int) else str(num_embeddings)\n",
        "                model_display = model_name.split('/')[-1] if '/' in model_name else model_name\n",
        "                \n",
        "                gr.Markdown(f\"\"\"\n",
        "                <div class=\"dataset-card\">\n",
        "                <h3 style=\"margin: 0 0 1rem 0; color: #374151; font-size: 1.25rem; font-weight: 700;\">üìä Dataset Information</h3>\n",
        "                <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 0.75rem; font-size: 0.875rem;\">\n",
        "                    <div><strong>üì∏ Images:</strong> {images_text}</div>\n",
        "                    <div><strong>üî¢ Embeddings:</strong> {embeddings_text}</div>\n",
        "                    <div><strong>üìê Dimension:</strong> {embedding_dim}D</div>\n",
        "                    <div><strong>ü§ñ Model:</strong> {model_display}</div>\n",
        "                    <div><strong>üìÅ Dataset:</strong> {dataset}</div>\n",
        "                    <div><strong>üìÖ Date:</strong> {processing_date}</div>\n",
        "                </div>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"\"\"\n",
        "                <div class=\"dataset-card\">\n",
        "                <h3 style=\"margin: 0 0 1rem 0; color: #374151; font-size: 1.25rem; font-weight: 700;\">üî• Popular Searches</h3>\n",
        "                <p style=\"margin: 0 0 1rem 0; color: #6b7280; font-size: 0.875rem;\">Try these popular search terms:</p>\n",
        "                \"\"\")\n",
        "                \n",
        "                # Create a simple list of popular searches\n",
        "                popular_text = '<div style=\"display: flex; flex-wrap: wrap; gap: 0.5rem;\">'\n",
        "                for search in popular_searches:\n",
        "                    popular_text += f'<span style=\"background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%); color: white; padding: 0.5rem 1rem; border-radius: 12px; font-size: 0.875rem; font-weight: 600; display: inline-block; margin: 0.25rem;\">{search}</span>'\n",
        "                popular_text += '</div></div>'\n",
        "                \n",
        "                gr.Markdown(popular_text)\n",
        "\n",
        "        # Main search interface\n",
        "        with gr.Tabs():\n",
        "            # Text-to-Image Search Tab\n",
        "            with gr.Tab(\"üî§ Text-to-Image Search\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                <div class=\"card\">\n",
        "                <h2 style=\"margin: 0 0 1rem 0; color: #374151; font-size: 1.5rem; font-weight: 700;\">üî§ Text-to-Image Search</h2>\n",
        "                <p style=\"margin: 0 0 2rem 0; color: #6b7280; font-size: 1rem;\">Describe what you're looking for and discover relevant images from the dataset</p>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        text_query = gr.Textbox(\n",
        "                            label=\"üîç Search Query\",\n",
        "                            placeholder=\"e.g., 'a dog playing in the park' or 'children smiling'\",\n",
        "                            info=\"üí° Be specific! Try describing objects, actions, colors, or emotions. The more descriptive, the better the results!\",\n",
        "                            elem_classes=[\"input\"]\n",
        "                        )\n",
        "                        num_results_text = gr.Slider(\n",
        "                            label=\"üìä Number of Results\",\n",
        "                            minimum=1,\n",
        "                            maximum=20,\n",
        "                            value=5,\n",
        "                            step=1,\n",
        "                            info=\"Choose how many results to display\"\n",
        "                        )\n",
        "                        search_btn = gr.Button(\"üîç Search Images\", variant=\"primary\", elem_classes=[\"btn\", \"btn-primary\"])\n",
        "\n",
        "                    with gr.Column(scale=1):\n",
        "                        gr.Markdown(\"\"\"\n",
        "                        <div class=\"dataset-card\">\n",
        "                        <h3 style=\"margin: 0 0 1rem 0; color: #374151; font-size: 1.125rem; font-weight: 700;\">üî• Popular Searches</h3>\n",
        "                        <p style=\"margin: 0 0 1rem 0; color: #6b7280; font-size: 0.875rem;\">Click any suggestion to search:</p>\n",
        "                        \"\"\")\n",
        "                        \n",
        "                        # Create clickable search suggestion buttons\n",
        "                        with gr.Row():\n",
        "                            with gr.Column():\n",
        "                                for i, search in enumerate(popular_searches[:4]):  # First 4 searches\n",
        "                                    btn = gr.Button(\n",
        "                                        f\"üîç {search}\", \n",
        "                                        size=\"sm\", \n",
        "                                        variant=\"secondary\",\n",
        "                                        elem_classes=[\"btn\"]\n",
        "                                    )\n",
        "                                    btn.click(\n",
        "                                        lambda s=search: s, \n",
        "                                        outputs=text_query\n",
        "                                    )\n",
        "                        \n",
        "                        with gr.Row():\n",
        "                            with gr.Column():\n",
        "                                for i, search in enumerate(popular_searches[4:]):  # Last 4 searches\n",
        "                                    btn = gr.Button(\n",
        "                                        f\"üîç {search}\", \n",
        "                                        size=\"sm\", \n",
        "                                        variant=\"secondary\",\n",
        "                                        elem_classes=[\"btn\"]\n",
        "                                    )\n",
        "                                    btn.click(\n",
        "                                        lambda s=search: s, \n",
        "                                        outputs=text_query\n",
        "                                    )\n",
        "                        \n",
        "                        gr.Markdown(\"\"\"\n",
        "                        <div class=\"dataset-card\" style=\"margin-top: 1rem;\">\n",
        "                        <h3 style=\"margin: 0 0 1rem 0; color: #374151; font-size: 1.125rem; font-weight: 700;\">üí° Search Tips</h3>\n",
        "                        <div style=\"color: #6b7280; font-size: 0.875rem; line-height: 1.6;\">\n",
        "                        <strong>Try searching for:</strong><br>\n",
        "                        ‚Ä¢ <strong>Animals:</strong> 'dog', 'cat', 'bird', 'horse'<br>\n",
        "                        ‚Ä¢ <strong>Activities:</strong> 'playing', 'running', 'cooking'<br>\n",
        "                        ‚Ä¢ <strong>Objects:</strong> 'car', 'house', 'food'<br>\n",
        "                        ‚Ä¢ <strong>Emotions:</strong> 'smiling', 'happy', 'sad'<br>\n",
        "                        ‚Ä¢ <strong>Scenes:</strong> 'beach', 'park', 'kitchen'\n",
        "                        </div>\n",
        "                        </div>\n",
        "                        \"\"\")\n",
        "\n",
        "                # Results\n",
        "                text_results = gr.Gallery(\n",
        "                    label=\"üé® Search Results\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"gallery\",\n",
        "                    columns=3,\n",
        "                    rows=2,\n",
        "                    object_fit=\"contain\",\n",
        "                    height=\"auto\",\n",
        "                    elem_classes=[\"gallery\"]\n",
        "                )\n",
        "                text_status = gr.Textbox(\n",
        "                    label=\"üìä Status\", \n",
        "                    interactive=False,\n",
        "                    elem_classes=[\"status\"]\n",
        "                )\n",
        "\n",
        "                # Connect search button\n",
        "                search_btn.click(\n",
        "                    search_images,\n",
        "                    inputs=[text_query, num_results_text],\n",
        "                    outputs=[text_results, text_status]\n",
        "                )\n",
        "\n",
        "            # Image-to-Text Search Tab\n",
        "            with gr.Tab(\"üñºÔ∏è Image-to-Text Search\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                <div class=\"card\">\n",
        "                <h2 style=\"margin: 0 0 1rem 0; color: #374151; font-size: 1.5rem; font-weight: 700;\">üñºÔ∏è Image-to-Text Search</h2>\n",
        "                <p style=\"margin: 0 0 2rem 0; color: #6b7280; font-size: 1rem;\">Upload an image to find similar text descriptions from the dataset</p>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        image_input = gr.Image(\n",
        "                            label=\"üìÅ Upload Image\",\n",
        "                            type=\"pil\",\n",
        "                            info=\"üí° Upload a clear image with a main subject for best search results!\",\n",
        "                            elem_classes=[\"input\"]\n",
        "                        )\n",
        "                        num_results_image = gr.Slider(\n",
        "                            label=\"üìä Number of Results\",\n",
        "                            minimum=1,\n",
        "                            maximum=20,\n",
        "                            value=5,\n",
        "                            step=1,\n",
        "                            info=\"Choose how many results to display\"\n",
        "                        )\n",
        "                        search_img_btn = gr.Button(\"üîç Search Descriptions\", variant=\"primary\", elem_classes=[\"btn\", \"btn-primary\"])\n",
        "\n",
        "                    with gr.Column(scale=1):\n",
        "                        gr.Markdown(\"\"\"\n",
        "                        <div class=\"dataset-card\">\n",
        "                        <h3 style=\"margin: 0 0 1rem 0; color: #374151; font-size: 1.125rem; font-weight: 700;\">üìã Upload Guidelines</h3>\n",
        "                        <div style=\"color: #6b7280; font-size: 0.875rem; line-height: 1.6;\">\n",
        "                        <strong>Supported formats:</strong><br>\n",
        "                        ‚Ä¢ JPG, JPEG<br>\n",
        "                        ‚Ä¢ PNG<br>\n",
        "                        ‚Ä¢ BMP, GIF<br><br>\n",
        "                        \n",
        "                        <strong>Best results with:</strong><br>\n",
        "                        ‚Ä¢ Clear, well-lit images<br>\n",
        "                        ‚Ä¢ Single main subject<br>\n",
        "                        ‚Ä¢ Good contrast\n",
        "                        </div>\n",
        "                        </div>\n",
        "                        \"\"\")\n",
        "\n",
        "                # Results\n",
        "                image_results = gr.Gallery(\n",
        "                    label=\"üé® Search Results\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"gallery\",\n",
        "                    columns=3,\n",
        "                    rows=2,\n",
        "                    object_fit=\"contain\",\n",
        "                    height=\"auto\",\n",
        "                    elem_classes=[\"gallery\"]\n",
        "                )\n",
        "                image_status = gr.Textbox(\n",
        "                    label=\"üìä Status\", \n",
        "                    interactive=False,\n",
        "                    elem_classes=[\"status\"]\n",
        "                )\n",
        "\n",
        "                # Connect search button\n",
        "                search_img_btn.click(\n",
        "                    search_descriptions,\n",
        "                    inputs=[image_input, num_results_image],\n",
        "                    outputs=[image_results, image_status]\n",
        "                )\n",
        "\n",
        "        # Footer\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        **üîç Search Engine** - Built with Gradio and OpenAI CLIP\n",
        "        \"\"\")\n",
        "\n",
        "    return app\n",
        "\n",
        "# Create and launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_gradio_app()\n",
        "    app.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        share=False,\n",
        "        show_error=True\n",
        "    )\n",
        "\n",
        "'''\n",
        "\n",
        "# Write Gradio app to file\n",
        "with open('../gradio_app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(gradio_code)\n",
        "\n",
        "print(\"‚úÖ Gradio app created successfully!\")\n",
        "print(\"üìÅ File saved as: ../gradio_app.py\")\n",
        "print(\"üöÄ To run: python ../gradio_app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Framework Comparison and Launch Instructions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç MULTIMODAL SEARCH ENGINE - FRAMEWORK COMPARISON\n",
            "============================================================\n",
            "\n",
            "üìä STREAMLIT vs GRADIO\n",
            "------------------------------\n",
            "| Feature                | Streamlit | Gradio |\n",
            "|------------------------|-----------|--------|\n",
            "| Learning Curve         | Easier    | Medium |\n",
            "| UI Style               | Traditional| Modern|\n",
            "| Layout                 | Column-based| Tab-based|\n",
            "| Interactions           | Form-based| Event-driven|\n",
            "| Customization          | High      | Medium |\n",
            "| Performance            | Good      | Good   |\n",
            "| Community              | Large     | Growing|\n",
            "| Documentation          | Excellent | Good   |\n",
            "| Deployment             | Easy      | Easy   |\n",
            "\n",
            "üöÄ LAUNCH INSTRUCTIONS\n",
            "------------------------------\n",
            "1. STREAMLIT APP (Port 8501):\n",
            "   streamlit run streamlit_app.py\n",
            "\n",
            "2. GRADIO APP (Port 7860):\n",
            "   python gradio_app.py\n",
            "\n",
            "3. RUN BOTH SIMULTANEOUSLY:\n",
            "   - Open two terminal windows\n",
            "   - Run each command in separate terminal\n",
            "   - Access Streamlit at: http://localhost:8501\n",
            "   - Access Gradio at: http://localhost:7860\n",
            "\n",
            "‚úÖ BOTH APPS INCLUDE:\n",
            "------------------------------\n",
            "‚Ä¢ Text-to-Image Search\n",
            "‚Ä¢ Image-to-Text Search\n",
            "‚Ä¢ Popular search suggestions\n",
            "‚Ä¢ Dataset information display\n",
            "‚Ä¢ Search tips and guidelines\n",
            "‚Ä¢ Real-time similarity scoring\n",
            "‚Ä¢ Responsive image galleries\n",
            "‚Ä¢ Error handling and validation\n",
            "\n",
            "üéØ RECOMMENDATION:\n",
            "------------------------------\n",
            "‚Ä¢ Use Streamlit for: Traditional web apps, data science projects\n",
            "‚Ä¢ Use Gradio for: AI demos, quick prototypes, modern interfaces\n",
            "‚Ä¢ Both are excellent choices for this project!\n"
          ]
        }
      ],
      "source": [
        "# Framework Comparison\n",
        "print(\"üîç MULTIMODAL SEARCH ENGINE - FRAMEWORK COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä STREAMLIT vs GRADIO\")\n",
        "print(\"-\" * 30)\n",
        "print(\"| Feature                | Streamlit | Gradio |\")\n",
        "print(\"|------------------------|-----------|--------|\")\n",
        "print(\"| Learning Curve         | Easier    | Medium |\")\n",
        "print(\"| UI Style               | Traditional| Modern|\")\n",
        "print(\"| Layout                 | Column-based| Tab-based|\")\n",
        "print(\"| Interactions           | Form-based| Event-driven|\")\n",
        "print(\"| Customization          | High      | Medium |\")\n",
        "print(\"| Performance            | Good      | Good   |\")\n",
        "print(\"| Community              | Large     | Growing|\")\n",
        "print(\"| Documentation          | Excellent | Good   |\")\n",
        "print(\"| Deployment             | Easy      | Easy   |\")\n",
        "\n",
        "print(\"\\nüöÄ LAUNCH INSTRUCTIONS\")\n",
        "print(\"-\" * 30)\n",
        "print(\"1. STREAMLIT APP (Port 8501):\")\n",
        "print(\"   streamlit run streamlit_app.py\")\n",
        "print()\n",
        "print(\"2. GRADIO APP (Port 7860):\")\n",
        "print(\"   python gradio_app.py\")\n",
        "print()\n",
        "print(\"3. RUN BOTH SIMULTANEOUSLY:\")\n",
        "print(\"   - Open two terminal windows\")\n",
        "print(\"   - Run each command in separate terminal\")\n",
        "print(\"   - Access Streamlit at: http://localhost:8501\")\n",
        "print(\"   - Access Gradio at: http://localhost:7860\")\n",
        "\n",
        "print(\"\\n‚úÖ BOTH APPS INCLUDE:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"‚Ä¢ Text-to-Image Search\")\n",
        "print(\"‚Ä¢ Image-to-Text Search\") \n",
        "print(\"‚Ä¢ Popular search suggestions\")\n",
        "print(\"‚Ä¢ Dataset information display\")\n",
        "print(\"‚Ä¢ Search tips and guidelines\")\n",
        "print(\"‚Ä¢ Real-time similarity scoring\")\n",
        "print(\"‚Ä¢ Responsive image galleries\")\n",
        "print(\"‚Ä¢ Error handling and validation\")\n",
        "\n",
        "print(\"\\nüéØ RECOMMENDATION:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"‚Ä¢ Use Streamlit for: Traditional web apps, data science projects\")\n",
        "print(\"‚Ä¢ Use Gradio for: AI demos, quick prototypes, modern interfaces\")\n",
        "print(\"‚Ä¢ Both are excellent choices for this project!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
