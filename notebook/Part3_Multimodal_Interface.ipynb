{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Multimodal & Interface Upgrade (Advanced continuation)\n",
        "\n",
        "## Project Overview\n",
        "This notebook demonstrates the creation of both Streamlit and Gradio web applications for our multimodal search engine. The search engine can:\n",
        "- Find images using text descriptions (text-to-image search)\n",
        "- Find text descriptions using uploaded images (image-to-text search)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. System Information and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîç MULTIMODAL INTERFACE - SYSTEM STATUS\n",
            "================================================================================\n",
            "üìÖ Timestamp: 2025-09-10 21:19:50\n",
            "\n",
            "üñ•Ô∏è  SYSTEM INFORMATION\n",
            "----------------------------------------\n",
            "Platform: Windows-11-10.0.26100-SP0\n",
            "Architecture: 64bit\n",
            "Processor: Intel64 Family 6 Model 151 Stepping 5, GenuineIntel\n",
            "Python Version: 3.12.9\n",
            "PyTorch Version: 2.8.0+cpu\n",
            "Streamlit Version: 1.49.1\n",
            "\n",
            "‚ö° HARDWARE INFORMATION\n",
            "----------------------------------------\n",
            "CPU Cores: 6 physical, 12 logical\n",
            "RAM: 15.8 GB total, 2.0 GB available\n",
            "RAM Usage: 87.7%\n",
            "Device: cpu\n",
            "GPU: Not available (using CPU)\n",
            "\n",
            "üìÅ PROJECT STATUS\n",
            "----------------------------------------\n",
            "‚úÖ Data directory found\n",
            "‚úÖ 8091 images found\n",
            "   Total image size: 1063.1 MB\n",
            "‚úÖ 40460 captions found (3355.2 KB)\n",
            "‚úÖ Flickr8k token file found (3315.7 KB)\n",
            "‚úÖ Embeddings directory found\n",
            "‚úÖ Image embeddings found (1.0 MB)\n",
            "‚úÖ Text embeddings found (1.0 MB)\n",
            "‚úÖ Metadata found (58.3 KB)\n",
            "‚úÖ Model info found\n",
            "\n",
            "üöÄ READY TO BUILD MULTIMODAL INTERFACE\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "import platform\n",
        "import psutil\n",
        "import sys\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Display comprehensive system information\n",
        "print(\"=\" * 80)\n",
        "print(\"üîç MULTIMODAL INTERFACE - SYSTEM STATUS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print()\n",
        "\n",
        "# System Information\n",
        "print(\"üñ•Ô∏è  SYSTEM INFORMATION\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "print(f\"Architecture: {platform.architecture()[0]}\")\n",
        "print(f\"Processor: {platform.processor()}\")\n",
        "print(f\"Python Version: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Streamlit Version: {st.__version__}\")\n",
        "print()\n",
        "\n",
        "# Hardware Information\n",
        "print(\"‚ö° HARDWARE INFORMATION\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
        "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB total, {psutil.virtual_memory().available / (1024**3):.1f} GB available\")\n",
        "print(f\"RAM Usage: {psutil.virtual_memory().percent:.1f}%\")\n",
        "\n",
        "# GPU Information\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
        "else:\n",
        "    print(\"GPU: Not available (using CPU)\")\n",
        "print()\n",
        "\n",
        "# Project Status\n",
        "print(\"üìÅ PROJECT STATUS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check if data exists\n",
        "data_path = '../data/'\n",
        "if os.path.exists(data_path):\n",
        "    print(\"‚úÖ Data directory found\")\n",
        "    if os.path.exists('../data/images/'):\n",
        "        image_files = [f for f in os.listdir('../data/images/') if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        image_count = len(image_files)\n",
        "        print(f\"‚úÖ {image_count} images found\")\n",
        "        if image_count > 0:\n",
        "            total_size = sum(os.path.getsize(os.path.join('../data/images/', f)) for f in image_files) / (1024**2)\n",
        "            print(f\"   Total image size: {total_size:.1f} MB\")\n",
        "    if os.path.exists('../data/captions.txt'):\n",
        "        with open('../data/captions.txt', 'r') as f:\n",
        "            caption_count = sum(1 for line in f)\n",
        "        caption_size = os.path.getsize('../data/captions.txt') / 1024\n",
        "        print(f\"‚úÖ {caption_count} captions found ({caption_size:.1f} KB)\")\n",
        "    if os.path.exists('../data/Flickr8k.token.txt'):\n",
        "        token_size = os.path.getsize('../data/Flickr8k.token.txt') / 1024\n",
        "        print(f\"‚úÖ Flickr8k token file found ({token_size:.1f} KB)\")\n",
        "else:\n",
        "    print(\"‚ùå Data directory not found!\")\n",
        "\n",
        "# Check if embeddings exist\n",
        "embeddings_path = '../embeddings/'\n",
        "if os.path.exists(embeddings_path):\n",
        "    print(\"‚úÖ Embeddings directory found\")\n",
        "    if os.path.exists('../embeddings/image_embeddings.npy'):\n",
        "        image_emb_size = os.path.getsize('../embeddings/image_embeddings.npy') / (1024**2)\n",
        "        print(f\"‚úÖ Image embeddings found ({image_emb_size:.1f} MB)\")\n",
        "    if os.path.exists('../embeddings/text_embeddings.npy'):\n",
        "        text_emb_size = os.path.getsize('../embeddings/text_embeddings.npy') / (1024**2)\n",
        "        print(f\"‚úÖ Text embeddings found ({text_emb_size:.1f} MB)\")\n",
        "    if os.path.exists('../embeddings/metadata.csv'):\n",
        "        metadata_size = os.path.getsize('../embeddings/metadata.csv') / 1024\n",
        "        print(f\"‚úÖ Metadata found ({metadata_size:.1f} KB)\")\n",
        "    if os.path.exists('../embeddings/model_info.json'):\n",
        "        print(\"‚úÖ Model info found\")\n",
        "else:\n",
        "    print(\"‚ùå Embeddings directory not found - please run Part 1 first!\")\n",
        "\n",
        "print()\n",
        "print(\"üöÄ READY TO BUILD MULTIMODAL INTERFACE\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model and Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è  Using device: cpu\n",
            "üîÑ Loading CLIP model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ed64efea76041b9a5c5fa20cb40b45b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CLIP model loaded successfully!\n",
            "üîÑ Loading embeddings data...\n",
            "‚úÖ Embeddings data loaded successfully!\n",
            "üìä Image embeddings shape: (500, 512)\n",
            "üìä Text embeddings shape: (500, 512)\n",
            "üìä Metadata shape: (500, 3)\n",
            "üìä Model info: {'model_name': 'openai/clip-vit-base-patch32', 'embedding_dim': 512, 'num_samples': 500, 'num_images': 8091, 'total_embeddings': 500, 'device_used': 'cpu', 'processing_date': '2025-09-10', 'dataset': 'Flickr8k'}\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "# Load CLIP model\n",
        "print(\"üîÑ Loading CLIP model...\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "print(\"‚úÖ CLIP model loaded successfully!\")\n",
        "\n",
        "# Load embeddings data\n",
        "print(\"üîÑ Loading embeddings data...\")\n",
        "image_embeddings = np.load('../embeddings/image_embeddings.npy')\n",
        "text_embeddings = np.load('../embeddings/text_embeddings.npy')\n",
        "metadata = pd.read_csv('../embeddings/metadata.csv')\n",
        "\n",
        "# Load model info\n",
        "with open('../embeddings/model_info.json', 'r') as f:\n",
        "    model_info = json.load(f)\n",
        "\n",
        "print(\"‚úÖ Embeddings data loaded successfully!\")\n",
        "print(f\"üìä Image embeddings shape: {image_embeddings.shape}\")\n",
        "print(f\"üìä Text embeddings shape: {text_embeddings.shape}\")\n",
        "print(f\"üìä Metadata shape: {metadata.shape}\")\n",
        "print(f\"üìä Model info: {model_info}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Search Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Search functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Text-to-Image Search Function\n",
        "def text_to_image_search(query_text, top_k=5):\n",
        "    \"\"\"Search for images based on text query\"\"\"\n",
        "    # Generate embedding for text query\n",
        "    inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_text_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Calculate similarities with all image embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), image_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar images\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Image-to-Text Search Function\n",
        "def image_to_text_search(uploaded_image, top_k=5):\n",
        "    \"\"\"Search for text descriptions based on uploaded image\"\"\"\n",
        "    # Generate embedding for uploaded image\n",
        "    inputs = processor(images=uploaded_image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_image_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Calculate similarities with all text embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), text_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar text descriptions\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Search functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Search Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing text-to-image search...\n",
            "Query: 'a dog playing'\n",
            "Found 3 results:\n",
            "1. Similarity: 0.324\n",
            "   Caption: A black and white dog catches a toy in midair .\n",
            "   Image ID: 1072153132_53d2bb1b60\n",
            "\n",
            "2. Similarity: 0.324\n",
            "   Caption: A dog leaps while chasing a tennis ball through a grassy field .\n",
            "   Image ID: 1072153132_53d2bb1b60\n",
            "\n",
            "3. Similarity: 0.324\n",
            "   Caption: A dog and a tennis ball .\n",
            "   Image ID: 1072153132_53d2bb1b60\n",
            "\n",
            "‚úÖ Text-to-image search test completed!\n"
          ]
        }
      ],
      "source": [
        "# Test text-to-image search\n",
        "print(\"üîç Testing text-to-image search...\")\n",
        "test_query = \"a dog playing\"\n",
        "results = text_to_image_search(test_query, top_k=3)\n",
        "\n",
        "print(f\"Query: '{test_query}'\")\n",
        "print(f\"Found {len(results)} results:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"{i}. Similarity: {result['similarity']:.3f}\")\n",
        "    print(f\"   Caption: {result['caption']}\")\n",
        "    print(f\"   Image ID: {result['image_id']}\")\n",
        "    print()\n",
        "\n",
        "print(\"‚úÖ Text-to-image search test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Standalone Streamlit App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Streamlit app created successfully!\n",
            "üìÅ File saved as: ../streamlit_app.py\n",
            "üöÄ To run: streamlit run ../streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "# Create the complete Streamlit app code\n",
        "streamlit_code = '''\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CLIP model\n",
        "@st.cache_resource\n",
        "def load_clip_model():\n",
        "    \"\"\"Load CLIP model and processor\"\"\"\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    return model, processor\n",
        "\n",
        "# Load embeddings data\n",
        "@st.cache_data\n",
        "def load_embeddings_data():\n",
        "    \"\"\"Load pre-computed embeddings and metadata\"\"\"\n",
        "    # Load embeddings\n",
        "    image_embeddings = np.load('../embeddings/image_embeddings.npy')\n",
        "    text_embeddings = np.load('../embeddings/text_embeddings.npy')\n",
        "    \n",
        "    # Load metadata\n",
        "    metadata = pd.read_csv('../embeddings/metadata.csv')\n",
        "    \n",
        "    # Load model info\n",
        "    with open('../embeddings/model_info.json', 'r') as f:\n",
        "        model_info = json.load(f)\n",
        "    \n",
        "    return image_embeddings, text_embeddings, metadata, model_info\n",
        "\n",
        "# Load model and data\n",
        "model, processor = load_clip_model()\n",
        "image_embeddings, text_embeddings, metadata, model_info = load_embeddings_data()\n",
        "\n",
        "# Text-to-Image Search Function\n",
        "def text_to_image_search(query_text, top_k=5):\n",
        "    \"\"\"Search for images based on text query\"\"\"\n",
        "    # Generate embedding for text query\n",
        "    inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_text_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Calculate similarities with all image embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), image_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar images\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Image-to-Text Search Function\n",
        "def image_to_text_search(uploaded_image, top_k=5):\n",
        "    \"\"\"Search for text descriptions based on uploaded image\"\"\"\n",
        "    # Generate embedding for uploaded image\n",
        "    inputs = processor(images=uploaded_image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_image_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Calculate similarities with all text embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), text_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar text descriptions\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Main Streamlit app\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"üîç Search Engine\",\n",
        "        page_icon=\"üîç\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\"\n",
        "    )\n",
        "    \n",
        "    # Header\n",
        "    st.title(\"üîç Search Engine\")\n",
        "    st.markdown(\"A powerful multimodal search engine using OpenAI CLIP\")\n",
        "    \n",
        "    # Sidebar\n",
        "    st.sidebar.header(\"Search Options\")\n",
        "    \n",
        "    # Search type selection\n",
        "    search_type = st.sidebar.radio(\n",
        "        \"Choose search type:\",\n",
        "        [\"Text-to-Image Search\", \"Image-to-Text Search\"]\n",
        "    )\n",
        "    \n",
        "    # Number of results\n",
        "    top_k = st.sidebar.slider(\n",
        "        \"Number of results:\",\n",
        "        min_value=1,\n",
        "        max_value=20,\n",
        "        value=5,\n",
        "        help=\"Number of top results to display\"\n",
        "    )\n",
        "    \n",
        "    # Popular searches\n",
        "    st.sidebar.markdown(\"---\")\n",
        "    st.sidebar.markdown(\"### üî• Popular Searches\")\n",
        "    \n",
        "    popular_searches = [\n",
        "        \"dog playing\", \"children smiling\", \"red car\", \"food cooking\",\n",
        "        \"person running\", \"cat sleeping\", \"blue sky\", \"water beach\",\n",
        "        \"house building\", \"tree nature\", \"person walking\", \"animal pet\"\n",
        "    ]\n",
        "    \n",
        "    # Create clickable search suggestions\n",
        "    for i, search in enumerate(popular_searches):\n",
        "        if st.sidebar.button(f\"üîç {search}\", key=f\"popular_{i}\"):\n",
        "            st.session_state.popular_search = search\n",
        "            st.session_state.auto_search = True\n",
        "    \n",
        "    # Display dataset info\n",
        "    st.sidebar.markdown(\"---\")\n",
        "    st.sidebar.markdown(\"### üìä Dataset Information\")\n",
        "    \n",
        "    # Get values and format properly\n",
        "    num_images = model_info.get('num_images', 'Unknown')\n",
        "    num_embeddings = model_info.get('total_embeddings', model_info.get('num_samples', 'Unknown'))\n",
        "    embedding_dim = model_info.get('embedding_dim', 'Unknown')\n",
        "    model_name = model_info.get('model_name', 'Unknown')\n",
        "    dataset = model_info.get('dataset', 'Unknown')\n",
        "    processing_date = model_info.get('processing_date', datetime.now().strftime('%Y-%m-%d'))\n",
        "    \n",
        "    # Format numbers properly\n",
        "    images_text = f\"{num_images:,}\" if isinstance(num_images, int) else str(num_images)\n",
        "    embeddings_text = f\"{num_embeddings:,}\" if isinstance(num_embeddings, int) else str(num_embeddings)\n",
        "    model_display = model_name.split('/')[-1] if '/' in model_name else model_name\n",
        "    \n",
        "    st.sidebar.metric(\"Total Images\", images_text)\n",
        "    st.sidebar.metric(\"Total Embeddings\", embeddings_text)\n",
        "    st.sidebar.metric(\"Embedding Dimension\", f\"{embedding_dim}D\")\n",
        "    st.sidebar.metric(\"Model\", model_display)\n",
        "    st.sidebar.metric(\"Dataset\", dataset)\n",
        "    st.sidebar.metric(\"Processing Date\", processing_date)\n",
        "    \n",
        "    # Check if this is a demo dataset\n",
        "    num_images = model_info.get('num_images', len(metadata))\n",
        "    if isinstance(num_images, int) and num_images < 1000:\n",
        "        st.warning(f\"‚ö†Ô∏è **Demo Mode**: You're using a small subset ({num_images:,} images) of the full Flickr8k dataset. For production use, run the full dataset processing in Part 1 to get all 8,091 images.\")\n",
        "    \n",
        "    # Main content area\n",
        "    if search_type == \"Text-to-Image Search\":\n",
        "        st.header(\"üî§ Text-to-Image Search\")\n",
        "        st.markdown(\"Enter a text description to find similar images:\")\n",
        "        \n",
        "        # Search suggestions\n",
        "        st.markdown(\"#### üí° Search Tips\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.markdown(\"**Try searching for:**\")\n",
        "            st.markdown(\"‚Ä¢ Animals: 'dog', 'cat', 'bird'\")\n",
        "            st.markdown(\"‚Ä¢ Activities: 'playing', 'running', 'cooking'\")\n",
        "            st.markdown(\"‚Ä¢ Objects: 'car', 'house', 'food'\")\n",
        "            st.markdown(\"‚Ä¢ Emotions: 'smiling', 'happy', 'sad'\")\n",
        "        \n",
        "        with col2:\n",
        "            st.markdown(\"**Example queries:**\")\n",
        "            if st.button(\"üêï A dog playing\", key=\"example1\"):\n",
        "                st.session_state.example_query = \"a dog playing\"\n",
        "                st.session_state.auto_search = True\n",
        "            if st.button(\"üë∂ Children smiling\", key=\"example2\"):\n",
        "                st.session_state.example_query = \"children smiling\"\n",
        "                st.session_state.auto_search = True\n",
        "            if st.button(\"üöó Red car\", key=\"example3\"):\n",
        "                st.session_state.example_query = \"red car\"\n",
        "                st.session_state.auto_search = True\n",
        "            if st.button(\"üçï Food cooking\", key=\"example4\"):\n",
        "                st.session_state.example_query = \"food cooking\"\n",
        "                st.session_state.auto_search = True\n",
        "        \n",
        "        # Text input with better placeholder\n",
        "        query_text = st.text_input(\n",
        "            \"üîç Enter your search query:\",\n",
        "            placeholder=\"Describe what you're looking for... (e.g., 'a dog playing in the park', 'children smiling', 'red car on street')\",\n",
        "            help=\"üí° Be specific! Try describing objects, actions, colors, or emotions. The more descriptive, the better the results!\",\n",
        "            value=st.session_state.get('example_query', st.session_state.get('popular_search', '')),\n",
        "            key=\"search_input\"\n",
        "        )\n",
        "        \n",
        "        # Clear example queries after use\n",
        "        if 'example_query' in st.session_state:\n",
        "            del st.session_state.example_query\n",
        "        if 'popular_search' in st.session_state:\n",
        "            del st.session_state.popular_search\n",
        "        \n",
        "        # Check if we should auto-search (from popular searches or example queries)\n",
        "        should_search = st.session_state.get('auto_search', False)\n",
        "        if should_search:\n",
        "            st.session_state.auto_search = False  # Reset the flag\n",
        "            # Use example query if available, otherwise use popular search\n",
        "            query_text = st.session_state.get('example_query', st.session_state.get('popular_search', query_text))\n",
        "        \n",
        "        if st.button(\"üîç Search Images\", type=\"primary\") or should_search:\n",
        "            if query_text:\n",
        "                with st.spinner(\"Searching for images...\"):\n",
        "                    results = text_to_image_search(query_text, top_k)\n",
        "                \n",
        "                if results:\n",
        "                    st.success(f\"Found {len(results)} results for: '{query_text}'\")\n",
        "                    \n",
        "                    # Display results in columns\n",
        "                    cols = st.columns(min(3, len(results)))\n",
        "                    for i, result in enumerate(results):\n",
        "                        with cols[i % 3]:\n",
        "                            try:\n",
        "                                image_path = result['image_path']\n",
        "                                # Fix path - remove ../ if present\n",
        "                                if image_path.startswith('../'):\n",
        "                                    image_path = image_path[3:]  # Remove ../\n",
        "                                \n",
        "                                if os.path.exists(image_path):\n",
        "                                    image = Image.open(image_path)\n",
        "                                    st.image(image, caption=f\"Similarity: {result['similarity']:.3f}\", use_container_width=True)\n",
        "                                    \n",
        "                                    # Display details\n",
        "                                    st.markdown(f\"**Image ID:** {result['image_id']}\")\n",
        "                                    st.markdown(f\"**Caption:** {result['caption']}\")\n",
        "                                    st.markdown(f\"**Similarity:** {result['similarity']:.3f}\")\n",
        "                                else:\n",
        "                                    st.error(f\"Image not found: {image_path}\")\n",
        "                            except Exception as e:\n",
        "                                st.error(f\"Error loading image: {e}\")\n",
        "                    else:\n",
        "                        st.warning(\"No results found. Try a different search query.\")\n",
        "            else:\n",
        "                st.warning(\"Please enter a search query.\")\n",
        "    \n",
        "    else:  # Image-to-Text Search\n",
        "        st.header(\"üñºÔ∏è Image-to-Text Search\")\n",
        "        st.markdown(\"Upload an image to find similar text descriptions:\")\n",
        "        \n",
        "        # Upload guidance\n",
        "        st.markdown(\"#### üìã Upload Guidelines\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.markdown(\"**Supported formats:**\")\n",
        "            st.markdown(\"‚Ä¢ JPG, JPEG\")\n",
        "            st.markdown(\"‚Ä¢ PNG\")\n",
        "            st.markdown(\"‚Ä¢ BMP, GIF\")\n",
        "        \n",
        "        with col2:\n",
        "            st.markdown(\"**Best results with:**\")\n",
        "            st.markdown(\"‚Ä¢ Clear, well-lit images\")\n",
        "            st.markdown(\"‚Ä¢ Single main subject\")\n",
        "            st.markdown(\"‚Ä¢ Good contrast\")\n",
        "        \n",
        "        # Image upload\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"üìÅ Choose an image file:\",\n",
        "            type=['jpg', 'jpeg', 'png', 'bmp', 'gif'],\n",
        "            help=\"üí° Upload a clear image with a main subject for best search results!\",\n",
        "            label_visibility=\"collapsed\"\n",
        "        )\n",
        "        \n",
        "        if uploaded_file is not None:\n",
        "            # Display uploaded image\n",
        "            uploaded_image = Image.open(uploaded_file)\n",
        "            st.image(uploaded_image, caption=\"Uploaded Image\", use_container_width=True)\n",
        "            \n",
        "            if st.button(\"üîç Search Descriptions\", type=\"primary\"):\n",
        "                with st.spinner(\"Searching for similar descriptions...\"):\n",
        "                    results = image_to_text_search(uploaded_image, top_k)\n",
        "                \n",
        "                if results:\n",
        "                    st.success(f\"Found {len(results)} similar descriptions\")\n",
        "                    \n",
        "                    # Display results in columns\n",
        "                    cols = st.columns(min(3, len(results)))\n",
        "                    for i, result in enumerate(results):\n",
        "                        with cols[i % 3]:\n",
        "                            try:\n",
        "                                image_path = result['image_path']\n",
        "                                # Fix path - remove ../ if present\n",
        "                                if image_path.startswith('../'):\n",
        "                                    image_path = image_path[3:]  # Remove ../\n",
        "                                \n",
        "                                if os.path.exists(image_path):\n",
        "                                    original_image = Image.open(image_path)\n",
        "                                    st.image(original_image, caption=\"Original Image\", use_container_width=True)\n",
        "                                else:\n",
        "                                    st.error(f\"Original image not found: {image_path}\")\n",
        "                            except Exception as e:\n",
        "                                st.error(f\"Error loading original image: {e}\")\n",
        "                            \n",
        "                            # Display details\n",
        "                            st.markdown(f\"**Image ID:** {result['image_id']}\")\n",
        "                            st.markdown(f\"**Caption:** {result['caption']}\")\n",
        "                            st.markdown(f\"**Similarity:** {result['similarity']:.3f}\")\n",
        "                else:\n",
        "                    st.warning(\"No results found. Try a different image.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "# Write Streamlit app to file\n",
        "with open('../streamlit_app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"‚úÖ Streamlit app created successfully!\")\n",
        "print(\"üìÅ File saved as: ../streamlit_app.py\")\n",
        "print(\"üöÄ To run: streamlit run ../streamlit_app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Standalone Gradio App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Gradio app created successfully!\n",
            "üìÅ File saved as: ../gradio_app.py\n",
            "üöÄ To run: python ../gradio_app.py\n"
          ]
        }
      ],
      "source": [
        "# Create the complete Gradio app code\n",
        "gradio_code = '''\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CLIP model\n",
        "@gr.cache()\n",
        "def load_clip_model():\n",
        "    \"\"\"Load CLIP model and processor\"\"\"\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    return model, processor\n",
        "\n",
        "# Load embeddings data\n",
        "@gr.cache()\n",
        "def load_embeddings_data():\n",
        "    \"\"\"Load pre-computed embeddings and metadata\"\"\"\n",
        "    # Load embeddings\n",
        "    image_embeddings = np.load('../embeddings/image_embeddings.npy')\n",
        "    text_embeddings = np.load('../embeddings/text_embeddings.npy')\n",
        "    \n",
        "    # Load metadata\n",
        "    metadata = pd.read_csv('../embeddings/metadata.csv')\n",
        "    \n",
        "    # Load model info\n",
        "    with open('../embeddings/model_info.json', 'r') as f:\n",
        "        model_info = json.load(f)\n",
        "    \n",
        "    return image_embeddings, text_embeddings, metadata, model_info\n",
        "\n",
        "# Load model and data\n",
        "model, processor = load_clip_model()\n",
        "image_embeddings, text_embeddings, metadata, model_info = load_embeddings_data()\n",
        "\n",
        "# Text-to-Image Search Function\n",
        "def text_to_image_search(query_text, top_k=5):\n",
        "    \"\"\"Search for images based on text query\"\"\"\n",
        "    # Generate embedding for text query\n",
        "    inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_text_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Calculate similarities with all image embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), image_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar images\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Image-to-Text Search Function\n",
        "def image_to_text_search(uploaded_image, top_k=5):\n",
        "    \"\"\"Search for text descriptions based on uploaded image\"\"\"\n",
        "    # Generate embedding for uploaded image\n",
        "    inputs = processor(images=uploaded_image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.get_image_features(**inputs)\n",
        "        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Calculate similarities with all text embeddings\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), text_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar text descriptions\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'image_id': metadata.iloc[idx]['image_id'],\n",
        "            'image_path': metadata.iloc[idx]['image_path'],\n",
        "            'caption': metadata.iloc[idx]['caption'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Text-to-Image Search Interface\n",
        "def search_images(query, num_results):\n",
        "    \"\"\"Gradio interface for text-to-image search\"\"\"\n",
        "    if not query.strip():\n",
        "        return [], \"Please enter a search query.\"\n",
        "    \n",
        "    try:\n",
        "        results = text_to_image_search(query, num_results)\n",
        "        \n",
        "        if not results:\n",
        "            return [], \"No results found. Try a different search query.\"\n",
        "        \n",
        "        # Prepare images and captions for display\n",
        "        images = []\n",
        "        captions = []\n",
        "        \n",
        "        for result in results:\n",
        "            image_path = result['image_path']\n",
        "            # Fix path - remove ../ if present\n",
        "            if image_path.startswith('../'):\n",
        "                image_path = image_path[3:]  # Remove ../\n",
        "            \n",
        "            if os.path.exists(image_path):\n",
        "                images.append(image_path)\n",
        "                captions.append(f\"Similarity: {result['similarity']:.3f}\\\\nCaption: {result['caption']}\")\n",
        "            else:\n",
        "                images.append(None)\n",
        "                captions.append(f\"Image not found: {image_path}\")\n",
        "        \n",
        "        return images, f\"Found {len(results)} results for: '{query}'\"\n",
        "    \n",
        "    except Exception as e:\n",
        "        return [], f\"Error during search: {str(e)}\"\n",
        "\n",
        "# Image-to-Text Search Interface\n",
        "def search_descriptions(image, num_results):\n",
        "    \"\"\"Gradio interface for image-to-text search\"\"\"\n",
        "    if image is None:\n",
        "        return [], \"Please upload an image.\"\n",
        "    \n",
        "    try:\n",
        "        results = image_to_text_search(image, num_results)\n",
        "        \n",
        "        if not results:\n",
        "            return [], \"No results found. Try a different image.\"\n",
        "        \n",
        "        # Prepare images and captions for display\n",
        "        images = []\n",
        "        captions = []\n",
        "        \n",
        "        for result in results:\n",
        "            image_path = result['image_path']\n",
        "            # Fix path - remove ../ if present\n",
        "            if image_path.startswith('../'):\n",
        "                image_path = image_path[3:]  # Remove ../\n",
        "            \n",
        "            if os.path.exists(image_path):\n",
        "                images.append(image_path)\n",
        "                captions.append(f\"Similarity: {result['similarity']:.3f}\\\\nCaption: {result['caption']}\")\n",
        "            else:\n",
        "                images.append(None)\n",
        "                captions.append(f\"Image not found: {image_path}\")\n",
        "        \n",
        "        return images, f\"Found {len(results)} similar descriptions\"\n",
        "    \n",
        "    except Exception as e:\n",
        "        return [], f\"Error during search: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_gradio_app():\n",
        "    \"\"\"Create the Gradio web application\"\"\"\n",
        "    \n",
        "    # Project description\n",
        "    description = \"\"\"\n",
        "    # üîç Multimodal Search Engine\n",
        "    \n",
        "    A powerful search engine that can find images using text descriptions and find text descriptions using images.\n",
        "    \n",
        "    **Technology Stack:**\n",
        "    - **Model**: OpenAI CLIP (Contrastive Language-Image Pre-training)\n",
        "    - **Framework**: Gradio for web interface\n",
        "    - **Dataset**: Flickr8k (8,091 images with captions)\n",
        "    - **Embeddings**: 512-dimensional vector representations\n",
        "    - **Similarity**: Cosine similarity for matching\n",
        "    \n",
        "    **Features:**\n",
        "    - Text-to-Image Search: Describe what you're looking for\n",
        "    - Image-to-Text Search: Upload an image to find similar descriptions\n",
        "    - Real-time similarity scoring\n",
        "    - Interactive web interface\n",
        "    \"\"\"\n",
        "    \n",
        "    # Popular search suggestions\n",
        "    popular_searches = [\n",
        "        \"dog playing\", \"children smiling\", \"red car\", \"food cooking\",\n",
        "        \"person running\", \"cat sleeping\", \"blue sky\", \"water beach\"\n",
        "    ]\n",
        "    \n",
        "    with gr.Blocks(title=\"üîç Search Engine\", theme=gr.themes.Soft()) as app:\n",
        "        gr.Markdown(description)\n",
        "        \n",
        "        # Dataset information\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(f\"\"\"\n",
        "                ### üìä Dataset Information\n",
        "                - **Total Images**: {model_info.get('num_images', 'Unknown'):,}\n",
        "                - **Total Embeddings**: {model_info.get('total_embeddings', model_info.get('num_samples', 'Unknown'):,}\n",
        "                - **Embedding Dimension**: {model_info.get('embedding_dim', 'Unknown')}D\n",
        "                - **Model**: {model_info.get('model_name', 'Unknown').split('/')[-1]}\n",
        "                - **Dataset**: {model_info.get('dataset', 'Unknown')}\n",
        "                - **Processing Date**: {model_info.get('processing_date', 'Unknown')}\n",
        "                \"\"\")\n",
        "            \n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(f\"\"\"\n",
        "                ### üî• Popular Searches\n",
        "                Click any suggestion to search:\n",
        "                \"\"\")\n",
        "                # Create clickable search suggestions\n",
        "                for i, search in enumerate(popular_searches):\n",
        "                    if i % 2 == 0:\n",
        "                        with gr.Row():\n",
        "                            gr.Button(f\"üîç {search}\", size=\"sm\").click(\n",
        "                                lambda s=search: s, outputs=gr.Textbox(visible=False)\n",
        "                            ).then(\n",
        "                                search_images, \n",
        "                                inputs=[gr.Textbox(value=search, visible=False), gr.Slider(1, 20, 5)],\n",
        "                                outputs=[gr.Gallery(), gr.Textbox()]\n",
        "                            )\n",
        "                    else:\n",
        "                        gr.Button(f\"üîç {search}\", size=\"sm\").click(\n",
        "                            lambda s=search: s, outputs=gr.Textbox(visible=False)\n",
        "                        ).then(\n",
        "                            search_images,\n",
        "                            inputs=[gr.Textbox(value=search, visible=False), gr.Slider(1, 20, 5)],\n",
        "                            outputs=[gr.Gallery(), gr.Textbox()]\n",
        "                        )\n",
        "        \n",
        "        # Main search interface\n",
        "        with gr.Tabs():\n",
        "            # Text-to-Image Search Tab\n",
        "            with gr.Tab(\"üî§ Text-to-Image Search\"):\n",
        "                gr.Markdown(\"Enter a text description to find similar images:\")\n",
        "                \n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        text_query = gr.Textbox(\n",
        "                            label=\"Search Query\",\n",
        "                            placeholder=\"e.g., 'a dog playing in the park' or 'children smiling'\",\n",
        "                            info=\"Describe what you're looking for in the images\"\n",
        "                        )\n",
        "                        num_results_text = gr.Slider(\n",
        "                            label=\"Number of Results\",\n",
        "                            minimum=1,\n",
        "                            maximum=20,\n",
        "                            value=5,\n",
        "                            step=1\n",
        "                        )\n",
        "                        search_btn = gr.Button(\"üîç Search Images\", variant=\"primary\")\n",
        "                    \n",
        "                    with gr.Column(scale=1):\n",
        "                        gr.Markdown(\"\"\"\n",
        "                        ### üí° Search Tips\n",
        "                        **Try searching for:**\n",
        "                        - Animals: 'dog', 'cat', 'bird'\n",
        "                        - Activities: 'playing', 'running', 'cooking'\n",
        "                        - Objects: 'car', 'house', 'food'\n",
        "                        - Emotions: 'smiling', 'happy', 'sad'\n",
        "                        \"\"\")\n",
        "                \n",
        "                # Results\n",
        "                text_results = gr.Gallery(\n",
        "                    label=\"Search Results\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"gallery\",\n",
        "                    columns=3,\n",
        "                    rows=2,\n",
        "                    object_fit=\"contain\",\n",
        "                    height=\"auto\"\n",
        "                )\n",
        "                text_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "                \n",
        "                # Connect search button\n",
        "                search_btn.click(\n",
        "                    search_images,\n",
        "                    inputs=[text_query, num_results_text],\n",
        "                    outputs=[text_results, text_status]\n",
        "                )\n",
        "            \n",
        "            # Image-to-Text Search Tab\n",
        "            with gr.Tab(\"üñºÔ∏è Image-to-Text Search\"):\n",
        "                gr.Markdown(\"Upload an image to find similar text descriptions:\")\n",
        "                \n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        image_input = gr.Image(\n",
        "                            label=\"Upload Image\",\n",
        "                            type=\"pil\",\n",
        "                            info=\"Upload a clear image with a main subject for best results\"\n",
        "                        )\n",
        "                        num_results_image = gr.Slider(\n",
        "                            label=\"Number of Results\",\n",
        "                            minimum=1,\n",
        "                            maximum=20,\n",
        "                            value=5,\n",
        "                            step=1\n",
        "                        )\n",
        "                        search_img_btn = gr.Button(\"üîç Search Descriptions\", variant=\"primary\")\n",
        "                    \n",
        "                    with gr.Column(scale=1):\n",
        "                        gr.Markdown(\"\"\"\n",
        "                        ### üìã Upload Guidelines\n",
        "                        **Supported formats:**\n",
        "                        - JPG, JPEG\n",
        "                        - PNG\n",
        "                        - BMP, GIF\n",
        "                        \n",
        "                        **Best results with:**\n",
        "                        - Clear, well-lit images\n",
        "                        - Single main subject\n",
        "                        - Good contrast\n",
        "                        \"\"\")\n",
        "                \n",
        "                # Results\n",
        "                image_results = gr.Gallery(\n",
        "                    label=\"Search Results\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"gallery\",\n",
        "                    columns=3,\n",
        "                    rows=2,\n",
        "                    object_fit=\"contain\",\n",
        "                    height=\"auto\"\n",
        "                )\n",
        "                image_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "                \n",
        "                # Connect search button\n",
        "                search_img_btn.click(\n",
        "                    search_descriptions,\n",
        "                    inputs=[image_input, num_results_image],\n",
        "                    outputs=[image_results, image_status]\n",
        "                )\n",
        "        \n",
        "        # Footer\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        **üîç Search Engine** - Built with Gradio and OpenAI CLIP\n",
        "        \"\"\")\n",
        "    \n",
        "    return app\n",
        "\n",
        "# Create and launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_gradio_app()\n",
        "    app.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        share=False,\n",
        "        show_error=True\n",
        "    )\n",
        "'''\n",
        "\n",
        "# Write Gradio app to file\n",
        "with open('../gradio_app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(gradio_code)\n",
        "\n",
        "print(\"‚úÖ Gradio app created successfully!\")\n",
        "print(\"üìÅ File saved as: ../gradio_app.py\")\n",
        "print(\"üöÄ To run: python ../gradio_app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Framework Comparison and Launch Instructions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç MULTIMODAL SEARCH ENGINE - FRAMEWORK COMPARISON\n",
            "============================================================\n",
            "\n",
            "üìä STREAMLIT vs GRADIO\n",
            "------------------------------\n",
            "| Feature                | Streamlit | Gradio |\n",
            "|------------------------|-----------|--------|\n",
            "| Learning Curve         | Easier    | Medium |\n",
            "| UI Style               | Traditional| Modern|\n",
            "| Layout                 | Column-based| Tab-based|\n",
            "| Interactions           | Form-based| Event-driven|\n",
            "| Customization          | High      | Medium |\n",
            "| Performance            | Good      | Good   |\n",
            "| Community              | Large     | Growing|\n",
            "| Documentation          | Excellent | Good   |\n",
            "| Deployment             | Easy      | Easy   |\n",
            "\n",
            "üöÄ LAUNCH INSTRUCTIONS\n",
            "------------------------------\n",
            "1. STREAMLIT APP (Port 8501):\n",
            "   streamlit run streamlit_app.py\n",
            "\n",
            "2. GRADIO APP (Port 7860):\n",
            "   python gradio_app.py\n",
            "\n",
            "3. RUN BOTH SIMULTANEOUSLY:\n",
            "   - Open two terminal windows\n",
            "   - Run each command in separate terminal\n",
            "   - Access Streamlit at: http://localhost:8501\n",
            "   - Access Gradio at: http://localhost:7860\n",
            "\n",
            "‚úÖ BOTH APPS INCLUDE:\n",
            "------------------------------\n",
            "‚Ä¢ Text-to-Image Search\n",
            "‚Ä¢ Image-to-Text Search\n",
            "‚Ä¢ Popular search suggestions\n",
            "‚Ä¢ Dataset information display\n",
            "‚Ä¢ Search tips and guidelines\n",
            "‚Ä¢ Real-time similarity scoring\n",
            "‚Ä¢ Responsive image galleries\n",
            "‚Ä¢ Error handling and validation\n",
            "\n",
            "üéØ RECOMMENDATION:\n",
            "------------------------------\n",
            "‚Ä¢ Use Streamlit for: Traditional web apps, data science projects\n",
            "‚Ä¢ Use Gradio for: AI demos, quick prototypes, modern interfaces\n",
            "‚Ä¢ Both are excellent choices for this project!\n"
          ]
        }
      ],
      "source": [
        "# Framework Comparison\n",
        "print(\"üîç MULTIMODAL SEARCH ENGINE - FRAMEWORK COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä STREAMLIT vs GRADIO\")\n",
        "print(\"-\" * 30)\n",
        "print(\"| Feature                | Streamlit | Gradio |\")\n",
        "print(\"|------------------------|-----------|--------|\")\n",
        "print(\"| Learning Curve         | Easier    | Medium |\")\n",
        "print(\"| UI Style               | Traditional| Modern|\")\n",
        "print(\"| Layout                 | Column-based| Tab-based|\")\n",
        "print(\"| Interactions           | Form-based| Event-driven|\")\n",
        "print(\"| Customization          | High      | Medium |\")\n",
        "print(\"| Performance            | Good      | Good   |\")\n",
        "print(\"| Community              | Large     | Growing|\")\n",
        "print(\"| Documentation          | Excellent | Good   |\")\n",
        "print(\"| Deployment             | Easy      | Easy   |\")\n",
        "\n",
        "print(\"\\nüöÄ LAUNCH INSTRUCTIONS\")\n",
        "print(\"-\" * 30)\n",
        "print(\"1. STREAMLIT APP (Port 8501):\")\n",
        "print(\"   streamlit run streamlit_app.py\")\n",
        "print()\n",
        "print(\"2. GRADIO APP (Port 7860):\")\n",
        "print(\"   python gradio_app.py\")\n",
        "print()\n",
        "print(\"3. RUN BOTH SIMULTANEOUSLY:\")\n",
        "print(\"   - Open two terminal windows\")\n",
        "print(\"   - Run each command in separate terminal\")\n",
        "print(\"   - Access Streamlit at: http://localhost:8501\")\n",
        "print(\"   - Access Gradio at: http://localhost:7860\")\n",
        "\n",
        "print(\"\\n‚úÖ BOTH APPS INCLUDE:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"‚Ä¢ Text-to-Image Search\")\n",
        "print(\"‚Ä¢ Image-to-Text Search\") \n",
        "print(\"‚Ä¢ Popular search suggestions\")\n",
        "print(\"‚Ä¢ Dataset information display\")\n",
        "print(\"‚Ä¢ Search tips and guidelines\")\n",
        "print(\"‚Ä¢ Real-time similarity scoring\")\n",
        "print(\"‚Ä¢ Responsive image galleries\")\n",
        "print(\"‚Ä¢ Error handling and validation\")\n",
        "\n",
        "print(\"\\nüéØ RECOMMENDATION:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"‚Ä¢ Use Streamlit for: Traditional web apps, data science projects\")\n",
        "print(\"‚Ä¢ Use Gradio for: AI demos, quick prototypes, modern interfaces\")\n",
        "print(\"‚Ä¢ Both are excellent choices for this project!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
