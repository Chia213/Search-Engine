{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Data Preparation & Embedding\n",
        "\n",
        "This notebook implements the first part of the search engine project:\n",
        "1. Load the dataset\n",
        "2. Choose a pre-trained multimodal model\n",
        "3. Generate vector embeddings for images and text\n",
        "4. Store embeddings with metadata\n",
        "\n",
        "## Dataset: Flickr8k\n",
        "We'll use the Flickr8k dataset which contains 8,000 images with 5 captions each.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Explore Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data directory structure\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('data/images', exist_ok=True)\n",
        "os.makedirs('embeddings', exist_ok=True)\n",
        "\n",
        "print(\"Directory structure created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For this demo, we'll create a sample dataset structure\n",
        "# In a real scenario, you would download Flickr8k from the official source\n",
        "\n",
        "# Create sample captions file (this would normally be downloaded)\n",
        "sample_captions = [\n",
        "    \"A dog is running in the park\",\n",
        "    \"A cat is sitting on a windowsill\", \n",
        "    \"Children are playing in the playground\",\n",
        "    \"A beautiful sunset over the ocean\",\n",
        "    \"A person riding a bicycle on the street\",\n",
        "    \"A bird is flying in the sky\",\n",
        "    \"A car is parked in front of a house\",\n",
        "    \"A flower garden in full bloom\",\n",
        "    \"A mountain landscape with snow\",\n",
        "    \"A person cooking in the kitchen\"\n",
        "]\n",
        "\n",
        "# Create sample captions file\n",
        "with open('data/captions.txt', 'w') as f:\n",
        "    for i, caption in enumerate(sample_captions):\n",
        "        f.write(f\"{i+1:04d}.jpg {caption}\\n\")\n",
        "\n",
        "print(f\"Created sample captions file with {len(sample_captions)} entries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load captions data\n",
        "def load_captions(captions_file):\n",
        "    \"\"\"Load captions from file\"\"\"\n",
        "    captions = {}\n",
        "    with open(captions_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) == 2:\n",
        "                image_id, caption = parts\n",
        "                if image_id not in captions:\n",
        "                    captions[image_id] = []\n",
        "                captions[image_id].append(caption)\n",
        "    return captions\n",
        "\n",
        "# Load our sample captions\n",
        "captions_data = load_captions('data/captions.txt')\n",
        "print(f\"Loaded captions for {len(captions_data)} images\")\n",
        "print(\"\\nSample captions:\")\n",
        "for i, (img_id, caps) in enumerate(list(captions_data.items())[:3]):\n",
        "    print(f\"{img_id}: {caps[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Pre-trained Multimodal Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CLIP model - a powerful multimodal model from OpenAI\n",
        "print(\"Loading CLIP model...\")\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model loaded successfully: {model_name}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Sample Images for Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample images for demonstration\n",
        "# In a real scenario, you would have actual images from Flickr8k\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import random\n",
        "\n",
        "def create_sample_image(text, filename, size=(224, 224)):\n",
        "    \"\"\"Create a sample image with text for demonstration\"\"\"\n",
        "    # Create a random colored background\n",
        "    colors = [(255, 200, 200), (200, 255, 200), (200, 200, 255), \n",
        "              (255, 255, 200), (255, 200, 255), (200, 255, 255)]\n",
        "    bg_color = random.choice(colors)\n",
        "    \n",
        "    img = Image.new('RGB', size, bg_color)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Try to use a default font, fallback to basic if not available\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "    \n",
        "    # Draw text in the center\n",
        "    bbox = draw.textbbox((0, 0), text, font=font)\n",
        "    text_width = bbox[2] - bbox[0]\n",
        "    text_height = bbox[3] - bbox[1]\n",
        "    \n",
        "    x = (size[0] - text_width) // 2\n",
        "    y = (size[1] - text_height) // 2\n",
        "    \n",
        "    draw.text((x, y), text, fill=(0, 0, 0), font=font)\n",
        "    \n",
        "    # Save the image\n",
        "    img.save(f'data/images/{filename}')\n",
        "    return img\n",
        "\n",
        "# Create sample images for each caption\n",
        "sample_images = []\n",
        "for i, (img_id, captions) in enumerate(captions_data.items()):\n",
        "    img = create_sample_image(captions[0], img_id)\n",
        "    sample_images.append((img_id, captions[0], img))\n",
        "\n",
        "print(f\"Created {len(sample_images)} sample images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, (img_id, caption, img) in enumerate(sample_images[:10]):\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"{img_id}\\n{caption[:30]}...\", fontsize=8)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_image_embedding(image_path, model, processor):\n",
        "    \"\"\"Generate embedding for an image\"\"\"\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            image_features = model.get_image_features(**inputs)\n",
        "            # Normalize the features\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        \n",
        "        return image_features.cpu().numpy().flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_text_embedding(text, model, processor):\n",
        "    \"\"\"Generate embedding for text\"\"\"\n",
        "    try:\n",
        "        inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            text_features = model.get_text_features(**inputs)\n",
        "            # Normalize the features\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        \n",
        "        return text_features.cpu().numpy().flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text '{text}': {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Embedding functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for all images and captions\n",
        "print(\"Generating embeddings...\")\n",
        "\n",
        "embeddings_data = []\n",
        "\n",
        "for img_id, captions in tqdm(captions_data.items(), desc=\"Processing images\"):\n",
        "    image_path = f'data/images/{img_id}'\n",
        "    \n",
        "    # Generate image embedding\n",
        "    image_embedding = get_image_embedding(image_path, model, processor)\n",
        "    \n",
        "    if image_embedding is not None:\n",
        "        # Generate text embeddings for all captions of this image\n",
        "        for caption in captions:\n",
        "            text_embedding = get_text_embedding(caption, model, processor)\n",
        "            \n",
        "            if text_embedding is not None:\n",
        "                embeddings_data.append({\n",
        "                    'image_id': img_id,\n",
        "                    'image_path': image_path,\n",
        "                    'caption': caption,\n",
        "                    'image_embedding': image_embedding,\n",
        "                    'text_embedding': text_embedding\n",
        "                })\n",
        "\n",
        "print(f\"Generated embeddings for {len(embeddings_data)} image-caption pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Store Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert embeddings to numpy arrays for easier storage\n",
        "image_embeddings = np.array([item['image_embedding'] for item in embeddings_data])\n",
        "text_embeddings = np.array([item['text_embedding'] for item in embeddings_data])\n",
        "\n",
        "# Create metadata dataframe\n",
        "metadata = pd.DataFrame([{\n",
        "    'image_id': item['image_id'],\n",
        "    'image_path': item['image_path'],\n",
        "    'caption': item['caption']\n",
        "} for item in embeddings_data])\n",
        "\n",
        "print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
        "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
        "print(f\"Metadata shape: {metadata.shape}\")\n",
        "\n",
        "# Display sample metadata\n",
        "print(\"\\nSample metadata:\")\n",
        "print(metadata.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save embeddings and metadata\n",
        "np.save('embeddings/image_embeddings.npy', image_embeddings)\n",
        "np.save('embeddings/text_embeddings.npy', text_embeddings)\n",
        "metadata.to_csv('embeddings/metadata.csv', index=False)\n",
        "\n",
        "# Save model info\n",
        "model_info = {\n",
        "    'model_name': model_name,\n",
        "    'embedding_dim': image_embeddings.shape[1],\n",
        "    'num_samples': len(embeddings_data),\n",
        "    'device_used': str(device)\n",
        "}\n",
        "\n",
        "with open('embeddings/model_info.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(\"Embeddings and metadata saved successfully!\")\n",
        "print(f\"Model info: {model_info}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Verify Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and verify saved embeddings\n",
        "loaded_image_embeddings = np.load('embeddings/image_embeddings.npy')\n",
        "loaded_text_embeddings = np.load('embeddings/text_embeddings.npy')\n",
        "loaded_metadata = pd.read_csv('embeddings/metadata.csv')\n",
        "\n",
        "print(f\"Loaded image embeddings shape: {loaded_image_embeddings.shape}\")\n",
        "print(f\"Loaded text embeddings shape: {loaded_text_embeddings.shape}\")\n",
        "print(f\"Loaded metadata shape: {loaded_metadata.shape}\")\n",
        "\n",
        "# Verify embeddings are normalized\n",
        "image_norms = np.linalg.norm(loaded_image_embeddings, axis=1)\n",
        "text_norms = np.linalg.norm(loaded_text_embeddings, axis=1)\n",
        "\n",
        "print(f\"\\nImage embedding norms - min: {image_norms.min():.6f}, max: {image_norms.max():.6f}\")\n",
        "print(f\"Text embedding norms - min: {text_norms.min():.6f}, max: {text_norms.max():.6f}\")\n",
        "print(\"Embeddings are properly normalized!\" if np.allclose(image_norms, 1.0) and np.allclose(text_norms, 1.0) else \"Warning: Embeddings may not be normalized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Basic Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test basic similarity between image and text embeddings\n",
        "def test_similarity(image_idx, text_idx):\n",
        "    \"\"\"Test similarity between specific image and text embeddings\"\"\"\n",
        "    img_emb = loaded_image_embeddings[image_idx]\n",
        "    txt_emb = loaded_text_embeddings[text_idx]\n",
        "    \n",
        "    similarity = cosine_similarity([img_emb], [txt_emb])[0][0]\n",
        "    \n",
        "    print(f\"Image: {loaded_metadata.iloc[image_idx]['image_id']}\")\n",
        "    print(f\"Text: {loaded_metadata.iloc[text_idx]['caption']}\")\n",
        "    print(f\"Similarity: {similarity:.4f}\")\n",
        "    \n",
        "    return similarity\n",
        "\n",
        "# Test a few examples\n",
        "print(\"Testing image-text similarity:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i in range(min(3, len(loaded_metadata))):\n",
        "    test_similarity(i, i)  # Same image-caption pair\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Part 1 Complete! Summary:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"✓ Dataset loaded: {len(captions_data)} images with captions\")\n",
        "print(f\"✓ Model selected: {model_name}\")\n",
        "print(f\"✓ Embeddings generated: {len(embeddings_data)} image-text pairs\")\n",
        "print(f\"✓ Embeddings stored: {loaded_image_embeddings.shape[0]} samples\")\n",
        "print(f\"✓ Embedding dimension: {loaded_image_embeddings.shape[1]}\")\n",
        "print(f\"✓ Files created:\")\n",
        "print(f\"  - embeddings/image_embeddings.npy\")\n",
        "print(f\"  - embeddings/text_embeddings.npy\")\n",
        "print(f\"  - embeddings/metadata.csv\")\n",
        "print(f\"  - embeddings/model_info.json\")\n",
        "print(\"\\nReady for Part 2: Search Functionality!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
